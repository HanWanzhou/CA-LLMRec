#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒSASRecæ¨¡å‹ - ä¸“ä¸ºè¶…è½»é‡åŒ–æ•°æ®é›†è®¾è®¡
"""

import os
import sys
import time
import torch
import argparse
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append('./pre_train/sasrec')
from pre_train.sasrec.model import SASRec
from pre_train.sasrec.data_preprocess import *
from pre_train.sasrec.utils import *

# å°†Argsç±»ç§»åˆ°å‡½æ•°å¤–éƒ¨ä»¥æ”¯æŒpickleåºåˆ—åŒ–
class Args:
    def __init__(self, dataset_name, epochs=20, device='cuda'):
        self.dataset = dataset_name
        self.batch_size = 128  # ä¸A-LLMRecä¿æŒä¸€è‡´ï¼ŒGPUå¯ä»¥å¤„ç†æ›´å¤§batch
        self.lr = 0.001
        self.maxlen = 50
        self.hidden_units = 50
        self.num_blocks = 2
        self.num_epochs = epochs
        self.num_heads = 1
        self.dropout_rate = 0.5
        self.l2_emb = 0.0
        self.device = device
        self.inference_only = False
        self.state_dict_path = None

def train_sasrec_ultra_light(dataset_name, device='cuda', epochs=20):
    """ä¸ºè¶…è½»é‡åŒ–æ•°æ®é›†å¿«é€Ÿè®­ç»ƒSASRecæ¨¡å‹"""
    
    print(f"ğŸš€ å¼€å§‹ä¸ºæ•°æ®é›† {dataset_name} è®­ç»ƒSASRecæ¨¡å‹...")
    
    args = Args(dataset_name, epochs, device)
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_path = f'./data/amazon/{dataset_name}.txt'
    
    # å¦‚æœç›´æ¥è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾å¸¦æœ‰åç¼€çš„æ–‡ä»¶
    if not os.path.exists(data_path):
        import glob
        pattern = f'./data/amazon/{dataset_name}*.txt'
        matching_files = glob.glob(pattern)
        
        if matching_files:
            data_path = matching_files[0]
            # æ›´æ–°datasetåç§°ä¸ºå®é™…æ–‡ä»¶åï¼ˆå»æ‰.txtåç¼€ï¼‰
            dataset_name = os.path.basename(data_path).replace('.txt', '')
            print(f"ğŸ“ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
            print(f"ğŸ”„ æ›´æ–°æ•°æ®é›†åç§°ä¸º: {dataset_name}")
        else:
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            print(f"âŒ ä¹Ÿæœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {pattern}")
            return False
    
    try:
        # æ•°æ®é¢„å¤„ç† - ç›´æ¥ä»txtæ–‡ä»¶åŠ è½½æ•°æ®
        print("ğŸ“Š ä»txtæ–‡ä»¶åŠ è½½æ•°æ®...")
        dataset = data_partition(args.dataset, path=data_path)
        
        [user_train, user_valid, user_test, usernum, itemnum] = dataset
        print(f'ç”¨æˆ·æ•°: {usernum}, ç‰©å“æ•°: {itemnum}')
        
        # è®¡ç®—å¹³å‡åºåˆ—é•¿åº¦
        cc = 0.0
        for u in user_train:
            cc += len(user_train[u])
        print(f'å¹³å‡åºåˆ—é•¿åº¦: {cc / len(user_train):.2f}')
        
        # æ•°æ®åŠ è½½å™¨
        print("ğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        sampler = WarpSampler(user_train, usernum, itemnum, 
                             batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)
        
        # æ¨¡å‹åˆå§‹åŒ–
        print("ğŸ§  åˆå§‹åŒ–SASRecæ¨¡å‹...")
        model = SASRec(usernum, itemnum, args).to(args.device)
        
        # å‚æ•°åˆå§‹åŒ–
        for name, param in model.named_parameters():
            try:
                torch.nn.init.xavier_normal_(param.data)
            except:
                pass  # è·³è¿‡ä¸€ç»´å‚æ•°
        
        model.train()
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))
        
        # è®­ç»ƒå¾ªç¯
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        num_batch = len(user_train) // args.batch_size
        
        for epoch in range(1, args.num_epochs + 1):
            t0 = time.time()
            running_loss = 0.0
            
            progress_bar = tqdm(range(num_batch), desc=f'Epoch {epoch}/{args.num_epochs}')
            
            for step in progress_bar:
                u, seq, pos, neg = sampler.next_batch()
                u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)
                
                pos_logits, neg_logits = model(u, seq, pos, neg)
                pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)
                
                optimizer.zero_grad()
                indices = np.where(pos != 0)
                loss = torch.nn.BCEWithLogitsLoss()(pos_logits[indices], pos_labels[indices])
                loss += torch.nn.BCEWithLogitsLoss()(neg_logits[indices], neg_labels[indices])
                
                for param in model.item_emb.parameters():
                    loss += args.l2_emb * torch.norm(param)
                
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                progress_bar.set_postfix({'loss': f'{running_loss/(step+1):.4f}'})
            
            # æ¯10ä¸ªepochè¯„ä¼°ä¸€æ¬¡
            if epoch % 10 == 0 or epoch == args.num_epochs:
                model.eval()
                print(f'\nğŸ“Š Epoch {epoch} è¯„ä¼°...')
                t_test = evaluate(model, dataset, args)
                t_valid = evaluate_valid(model, dataset, args)
                
                print(f'Epoch: {epoch}, Valid (NDCG@10: {t_valid[0]:.4f}, HR@10: {t_valid[1]:.4f}), '
                      f'Test (NDCG@10: {t_test[0]:.4f}, HR@10: {t_test[1]:.4f})')
                model.train()
        
        # ä¿å­˜æ¨¡å‹ - å…¼å®¹A-LLMRecæ ¼å¼
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        # åŒæ—¶ä¿å­˜åˆ°ä¸¤ä¸ªä½ç½®ä»¥ç¡®ä¿å…¼å®¹æ€§
        # 1. ä¿å­˜åˆ°pre_trainç›®å½• (A-LLMRecæ ¼å¼)
        output_dir_a = f'./pre_train/sasrec/{args.dataset}'
        os.makedirs(output_dir_a, exist_ok=True)
        
        # 2. ä¿å­˜åˆ°outputç›®å½• (æˆ‘ä»¬çš„æ ¼å¼)
        output_dir_b = f'./pre_train/sasrec/output/{args.dataset}'
        os.makedirs(output_dir_b, exist_ok=True)
        
        fname = f'SASRec.epoch={args.num_epochs}.lr={args.lr}.layer={args.num_blocks}.head={args.num_heads}.hidden={args.hidden_units}.maxlen={args.maxlen}.pth'
        
        # ä¿å­˜åˆ°ä¸¤ä¸ªä½ç½®
        model_path_a = os.path.join(output_dir_a, fname)
        model_path_b = os.path.join(output_dir_b, fname)
        
        torch.save([model.kwargs, model.state_dict()], model_path_a)
        torch.save([model.kwargs, model.state_dict()], model_path_b)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°:")
        print(f"   ğŸ“ A-LLMRecå…¼å®¹ä½ç½®: {model_path_a}")
        print(f"   ğŸ“ å¤‡ä»½ä½ç½®: {model_path_b}")
        print(f"ğŸ”— æ­¤æ¨¡å‹å¯ç›´æ¥ç”¨äºA-LLMRecè®­ç»ƒ")
        
        sampler.close()
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿè®­ç»ƒSASRecæ¨¡å‹')
    parser.add_argument('--dataset', required=True, help='æ•°æ®é›†åç§°')
    parser.add_argument('--device', default='cuda', help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--epochs', default=20, type=int, help='è®­ç»ƒè½®æ•°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        args.device = 'cpu'
    
    success = train_sasrec_ultra_light(args.dataset, args.device, args.epochs)
    
    if success:
        print("ğŸ‰ SASRecæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    else:
        print("ğŸ’¥ SASRecæ¨¡å‹è®­ç»ƒå¤±è´¥!")
        sys.exit(1)

if __name__ == '__main__':
    main() 