#!/usr/bin/env python3
"""
CA-LLMRec è¶…è½»é‡åŒ–è®­ç»ƒè„šæœ¬
ä½¿ç”¨5%æ•°æ®é›†è¿›è¡Œå¿«é€Ÿè®­ç»ƒï¼Œç›®æ ‡6å°æ—¶å†…å®Œæˆå…¨éƒ¨è®­ç»ƒå’Œè¯„ä¼°
"""

import os
import sys
import torch
import argparse
import time
import json
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from utils import *
from train_model import *
from models.a_llmrec_model import A_llmrec_model


def create_ultra_light_args(max_users=1000, max_items=3000):
    """åˆ›å»ºè¶…è½»é‡åŒ–è®­ç»ƒå‚æ•°"""
    parser = argparse.ArgumentParser()
    
    # åŸºç¡€è®¾ç½®
    parser.add_argument("--device", type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument("--multi_gpu", action='store_false')  # å•GPUè®­ç»ƒ
    parser.add_argument('--gpu_num', type=int, default=0)
    
    # æ¨¡å‹è®¾ç½®
    parser.add_argument("--llm", type=str, default='opt', help='flan_t5, opt, vicuna')
    parser.add_argument("--recsys", type=str, default='sasrec')
    
    # è¶…è½»é‡åŒ–æ•°æ®é›†è®¾ç½®
    dataset_suffix = f"_ultra_light_{max_users}u_{max_items}i"
    parser.add_argument("--rec_pre_trained_data", type=str, 
                       default=f'Movies_and_TV{dataset_suffix}')
    
    # è®­ç»ƒé˜¶æ®µè®¾ç½®
    parser.add_argument("--pretrain_stage1", action='store_true')
    parser.add_argument("--pretrain_stage2", action='store_true')
    parser.add_argument("--inference", action='store_true')
    
    # è¶…è½»é‡åŒ–è¶…å‚æ•° - é’ˆå¯¹å¿«é€Ÿè®­ç»ƒä¼˜åŒ–
    parser.add_argument('--batch_size1', default=128, type=int)  # Stage 1å¤§æ‰¹æ¬¡
    parser.add_argument('--batch_size2', default=8, type=int)    # Stage 2é€‚ä¸­æ‰¹æ¬¡
    parser.add_argument('--batch_size_infer', default=16, type=int)
    parser.add_argument('--maxlen', default=50, type=int)        # ä¸é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸€è‡´
    parser.add_argument('--num_epochs', default=20, type=int)    # å‡å°‘è®­ç»ƒè½®æ•°
    parser.add_argument("--stage1_lr", type=float, default=0.001)  # æé«˜å­¦ä¹ ç‡
    parser.add_argument("--stage2_lr", type=float, default=0.0005) # æé«˜å­¦ä¹ ç‡
    
    # CA-LLMRecåäº‹å®å‚æ•° - ä¼˜åŒ–ç‰ˆ
    parser.add_argument("--enable_counterfactual", action='store_true',
                       help='Enable counterfactual augmented training')
    parser.add_argument("--cf_alignment_weight", type=float, default=0.05,
                       help='Weight for counterfactual alignment loss (é™ä½ä»¥åŠ é€Ÿæ”¶æ•›)')
    parser.add_argument("--cf_mask_ratio", type=float, default=0.15,
                       help='Mask ratio for counterfactual sequence generation (é™ä½å¤æ‚åº¦)')
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    parser.add_argument("--num_workers", type=int, default=4, help='DataLoader workers')
    parser.add_argument("--pin_memory", action='store_true', help='Pin memory for faster GPU transfer')
    parser.add_argument("--compile_model", action='store_true', help='Use torch.compile for acceleration')
    
    # æ—©åœå’Œè°ƒåº¦å™¨
    parser.add_argument("--early_stopping_patience", type=int, default=5, 
                       help='Early stopping patience')
    parser.add_argument("--use_scheduler", action='store_true', 
                       help='Use learning rate scheduler')
    
    return parser.parse_args(args=[])


def check_ultra_light_dataset(dataset_name):
    """æ£€æŸ¥è¶…è½»é‡åŒ–æ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    data_path = f'./data/amazon/{dataset_name}.txt'
    mapping_path = f'./data/amazon/{dataset_name}_mappings.json'
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {data_path}")
        print("è¯·å…ˆè¿è¡Œ: python create_ultra_light_dataset.py")
        return False
    
    if not os.path.exists(mapping_path):
        print(f"âŒ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_path}")
        return False
    
    # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
    try:
        with open(mapping_path, 'r') as f:
            mappings = json.load(f)
        
        stats = mappings['stats']
        print(f"âœ… æ•°æ®é›†æ£€æŸ¥é€šè¿‡:")
        print(f"  ç”¨æˆ·æ•°: {mappings['user_count']}")
        print(f"  ç‰©å“æ•°: {mappings['item_count']}")
        print(f"  äº¤äº’æ•°: {mappings['interaction_count']}")
        print(f"  ç”¨æˆ·å‹ç¼©ç‡: {stats['compression_ratio_users']:.1%}")
        print(f"  ç‰©å“å‹ç¼©ç‡: {stats['compression_ratio_items']:.1%}")
        
        return True
    except Exception as e:
        print(f"âŒ è¯»å–æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
        return False


def prepare_sasrec_for_ultra_light(args):
    """ä¸ºè¶…è½»é‡åŒ–æ•°æ®é›†å‡†å¤‡SASRecæ¨¡å‹"""
    
    # A-LLMRecæœŸæœ›çš„ç›®å½•ç»“æ„ï¼špre_train/sasrec/{dataset_name}/
    target_dir = f'./pre_train/sasrec/{args.rec_pre_trained_data}'
    
    # æ£€æŸ¥A-LLMRecæœŸæœ›çš„ç›®å½•æ˜¯å¦å­˜åœ¨å¹¶ä¸”æœ‰æ¨¡å‹æ–‡ä»¶
    if os.path.exists(target_dir):
        import glob
        pth_files = glob.glob(os.path.join(target_dir, '*.pth'))
        if pth_files:
            if len(pth_files) == 1:
                print(f"âœ… æ‰¾åˆ°A-LLMRecå…¼å®¹çš„SASRecæ¨¡å‹: {pth_files[0]}")
                return True
            else:
                print(f"âš ï¸ å‘ç°å¤šä¸ªæ¨¡å‹æ–‡ä»¶ï¼ŒA-LLMRecéœ€è¦å”¯ä¸€æ¨¡å‹æ–‡ä»¶:")
                for pth in pth_files:
                    print(f"   ğŸ“„ {pth}")
                print("è¯·ä¿ç•™æœ€å¥½çš„æ¨¡å‹ï¼Œåˆ é™¤å…¶ä»–æ¨¡å‹æ–‡ä»¶")
                return False
    
    # æ£€æŸ¥æˆ‘ä»¬çš„outputç›®å½•æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    output_dir = f'./pre_train/sasrec/output/{args.rec_pre_trained_data}'
    if os.path.exists(output_dir):
        import glob
        pth_files = glob.glob(os.path.join(output_dir, '*.pth'))
        if pth_files:
            print(f"ğŸ“‹ åœ¨outputç›®å½•å‘ç°è®­ç»ƒå¥½çš„SASRecæ¨¡å‹:")
            for pth in pth_files:
                print(f"   ğŸ“„ {pth}")
            
            # é€‰æ‹©epochæ•°æœ€é«˜çš„æ¨¡å‹
            best_model = None
            best_epoch = 0
            for pth in pth_files:
                filename = os.path.basename(pth)
                if 'epoch=' in filename:
                    try:
                        epoch_str = filename.split('epoch=')[1].split('.')[0]
                        epoch = int(epoch_str)
                        if epoch > best_epoch:
                            best_epoch = epoch
                            best_model = pth
                    except:
                        pass
            
            if best_model:
                print(f"ğŸ† é€‰æ‹©æœ€ä½³æ¨¡å‹ (epoch={best_epoch}): {best_model}")
                
                # åˆ›å»ºA-LLMRecæœŸæœ›çš„ç›®å½•
                os.makedirs(target_dir, exist_ok=True)
                
                # å¤åˆ¶æœ€ä½³æ¨¡å‹åˆ°A-LLMRecæœŸæœ›çš„ä½ç½®
                import shutil
                target_path = os.path.join(target_dir, os.path.basename(best_model))
                shutil.copy(best_model, target_path)
                print(f"âœ… æ¨¡å‹å·²å¤åˆ¶åˆ°A-LLMRecæœŸæœ›ä½ç½®: {target_path}")
                return True
    
    print("ğŸ”§ æœªæ‰¾åˆ°ç°æœ‰SASRecæ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒ...")
    
    # æ£€æŸ¥åŸå§‹Movies_and_TVæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆä½œä¸ºfallbackï¼‰
    fallback_dirs = [
        './pre_train/sasrec/output/Movies_and_TV',
        './pre_train/sasrec/Movies_and_TV'
    ]
    
    for fallback_dir in fallback_dirs:
        if os.path.exists(fallback_dir):
            import glob
            pth_files = glob.glob(os.path.join(fallback_dir, '*.pth'))
            if pth_files:
                print(f"ğŸ“‹ å‘ç°åŸå§‹SASRecæ¨¡å‹ï¼Œå°†å¤åˆ¶å¹¶è°ƒæ•´...")
                
                # é€‰æ‹©æœ€å¥½çš„æ¨¡å‹
                best_model = pth_files[0]  # ç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ª
                
                # åˆ›å»ºç›®å½•
                os.makedirs(target_dir, exist_ok=True)
                
                # å¤åˆ¶æ¨¡å‹
                import shutil
                target_path = os.path.join(target_dir, os.path.basename(best_model))
                shutil.copy(best_model, target_path)
                print(f"âœ… SASRecæ¨¡å‹å‡†å¤‡å®Œæˆ: {target_path}")
                return True
    
    print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•é¢„è®­ç»ƒSASRecæ¨¡å‹ï¼Œå¼€å§‹è‡ªåŠ¨è®­ç»ƒ...")
    
    # è‡ªåŠ¨è®­ç»ƒSASRecæ¨¡å‹
    try:
        from train_sasrec_ultra_light import train_sasrec_ultra_light
        
        # ä½¿ç”¨è¾ƒå°‘çš„epochsè¿›è¡Œå¿«é€Ÿè®­ç»ƒ
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        success = train_sasrec_ultra_light(args.rec_pre_trained_data, device=device, epochs=20)
        
        if success:
            print(f"âœ… SASRecæ¨¡å‹è‡ªåŠ¨è®­ç»ƒå®Œæˆ")
            return True
        else:
            print("âŒ SASRecæ¨¡å‹è‡ªåŠ¨è®­ç»ƒå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨è®­ç»ƒSASRecæ—¶å‡ºé”™: {e}")
        print("å»ºè®®æ‰‹åŠ¨è¿è¡Œ: python train_sasrec_ultra_light.py --dataset", args.rec_pre_trained_data)
        return False


def train_stage1_ultra_fast(args):
    """è¶…å¿«é€ŸStage 1è®­ç»ƒ"""
    print("\nğŸš€ å¼€å§‹Stage 1è®­ç»ƒ (è¶…è½»é‡åŒ–)")
    print("=" * 50)
    
    start_time = time.time()
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    args.pretrain_stage1 = True
    args.pretrain_stage2 = False
    args.inference = False
    
    try:
        # è°ƒç”¨åŸå§‹è®­ç»ƒå‡½æ•°
        if args.multi_gpu:
            train_model_phase1(args)
        else:
            train_model_phase1_(0, 1, args)
        
        stage1_time = time.time() - start_time
        print(f"\nâœ… Stage 1è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {stage1_time/60:.1f}åˆ†é’Ÿ")
        return True
        
    except Exception as e:
        print(f"âŒ Stage 1è®­ç»ƒå¤±è´¥: {e}")
        return False


def train_stage2_ultra_fast(args):
    """è¶…å¿«é€ŸStage 2è®­ç»ƒ"""
    print("\nğŸš€ å¼€å§‹Stage 2è®­ç»ƒ (è¶…è½»é‡åŒ–)")
    print("=" * 50)
    
    start_time = time.time()
    
    # è®¾ç½®è®­ç»ƒå‚æ•°
    args.pretrain_stage1 = False
    args.pretrain_stage2 = True
    args.inference = False
    
    try:
        # è°ƒç”¨åŸå§‹è®­ç»ƒå‡½æ•°
        if args.multi_gpu:
            train_model_phase2(args)
        else:
            train_model_phase2_(0, 1, args)
        
        stage2_time = time.time() - start_time
        print(f"\nâœ… Stage 2è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {stage2_time/60:.1f}åˆ†é’Ÿ")
        return True
        
    except Exception as e:
        print(f"âŒ Stage 2è®­ç»ƒå¤±è´¥: {e}")
        return False


def evaluate_ultra_fast(args):
    """è¶…å¿«é€Ÿè¯„ä¼°"""
    print("\nğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print("=" * 50)
    
    start_time = time.time()
    
    # è®¾ç½®è¯„ä¼°å‚æ•°
    args.pretrain_stage1 = False
    args.pretrain_stage2 = False
    args.inference = True
    
    try:
        # è°ƒç”¨åŸå§‹è¯„ä¼°å‡½æ•°
        inference(args)
        
        eval_time = time.time() - start_time
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {eval_time/60:.1f}åˆ†é’Ÿ")
        
        # è¿è¡Œè¯„ä¼°è„šæœ¬
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        os.system("python eval.py")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return False


def main():
    """è¶…è½»é‡åŒ–è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CA-LLMRecè¶…è½»é‡åŒ–è®­ç»ƒ')
    parser.add_argument('--max_users', type=int, default=1000, 
                       help='æœ€å¤§ç”¨æˆ·æ•°é‡ (é»˜è®¤: 1000)')
    parser.add_argument('--max_items', type=int, default=3000, 
                       help='æœ€å¤§ç‰©å“æ•°é‡ (é»˜è®¤: 3000)')
    parser.add_argument('--skip_stage1', action='store_true', 
                       help='è·³è¿‡Stage 1è®­ç»ƒ')
    parser.add_argument('--skip_stage2', action='store_true', 
                       help='è·³è¿‡Stage 2è®­ç»ƒ')
    parser.add_argument('--skip_eval', action='store_true', 
                       help='è·³è¿‡è¯„ä¼°')
    
    cmd_args = parser.parse_args()
    
    print("ğŸš€ CA-LLMRec è¶…è½»é‡åŒ–è®­ç»ƒ")
    print("=" * 60)
    print(f"ç›®æ ‡ï¼šä½¿ç”¨{cmd_args.max_users}ç”¨æˆ·ï¼Œ{cmd_args.max_items}ç‰©å“çš„æ•°æ®é›†")
    print(f"é¢„è®¡æ€»è®­ç»ƒæ—¶é—´ï¼š4-6å°æ—¶")
    
    # åˆ›å»ºè®­ç»ƒå‚æ•°
    args = create_ultra_light_args(cmd_args.max_users, cmd_args.max_items)
    
    # å¯ç”¨CA-LLMRecåŠŸèƒ½
    args.enable_counterfactual = True
    
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  æ•°æ®é›†: {args.rec_pre_trained_data}")
    print(f"  è®¾å¤‡: {args.device}")
    print(f"  åäº‹å®åŠŸèƒ½: {'âœ…' if args.enable_counterfactual else 'âŒ'}")
    print(f"  æ‰¹æ¬¡å¤§å°: Stage1={args.batch_size1}, Stage2={args.batch_size2}")
    print(f"  è®­ç»ƒè½®æ•°: {args.num_epochs}")
    print(f"  åºåˆ—é•¿åº¦: {args.maxlen}")
    
    # æ£€æŸ¥æ•°æ®é›†
    if not check_ultra_light_dataset(args.rec_pre_trained_data):
        print("\nè¯·å…ˆè¿è¡Œæ•°æ®é›†ç”Ÿæˆè„šæœ¬:")
        print(f"python create_ultra_light_dataset.py --max_users {cmd_args.max_users} --max_items {cmd_args.max_items}")
        return
    
    # å‡†å¤‡SASRecæ¨¡å‹
    if not prepare_sasrec_for_ultra_light(args):
        print("âš ï¸ SASRecæ¨¡å‹å‡†å¤‡å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
    
    # å¼€å§‹è®­ç»ƒ
    total_start_time = time.time()
    success_stages = []
    
    try:
        # Stage 1: ååŒè¿‡æ»¤åµŒå…¥å¯¹é½
        if not cmd_args.skip_stage1:
            print(f"\n{'='*60}")
            print("ğŸ¯ Stage 1: ååŒè¿‡æ»¤åµŒå…¥å¯¹é½ + åäº‹å®å¢å¼º")
            print(f"é¢„è®¡æ—¶é—´: 1-2å°æ—¶")
            
            if train_stage1_ultra_fast(args):
                success_stages.append("Stage 1")
            else:
                print("âŒ Stage 1è®­ç»ƒå¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
                return
        
        # Stage 2: LLMæ¨èè®­ç»ƒ
        if not cmd_args.skip_stage2:
            print(f"\n{'='*60}")
            print("ğŸ¯ Stage 2: LLMæ¨èè®­ç»ƒ")
            print(f"é¢„è®¡æ—¶é—´: 2-3å°æ—¶")
            
            if train_stage2_ultra_fast(args):
                success_stages.append("Stage 2")
            else:
                print("âŒ Stage 2è®­ç»ƒå¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­è¯„ä¼°Stage 1ç»“æœ")
        
        # è¯„ä¼°
        if not cmd_args.skip_eval:
            print(f"\n{'='*60}")
            print("ğŸ¯ æ¨¡å‹è¯„ä¼°")
            print(f"é¢„è®¡æ—¶é—´: 30åˆ†é’Ÿ")
            
            if evaluate_ultra_fast(args):
                success_stages.append("Evaluation")
            else:
                print("âŒ è¯„ä¼°å¤±è´¥")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
    
    # æ€»ç»“
    total_time = time.time() - total_start_time
    print(f"\n{'='*60}")
    print("ğŸ è®­ç»ƒå®Œæˆæ€»ç»“")
    print(f"æ€»è€—æ—¶: {total_time/3600:.1f}å°æ—¶ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æˆåŠŸé˜¶æ®µ: {', '.join(success_stages) if success_stages else 'æ— '}")
    
    if "Stage 1" in success_stages:
        print("âœ… åäº‹å®å¢å¼ºçš„ååŒè¿‡æ»¤åµŒå…¥è®­ç»ƒæˆåŠŸ")
    if "Stage 2" in success_stages:
        print("âœ… LLMæ¨èè®­ç»ƒæˆåŠŸ")
    if "Evaluation" in success_stages:
        print("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ")
        print("ğŸ“Š è¯·æŸ¥çœ‹evaluationç»“æœäº†è§£æ¨¡å‹æ€§èƒ½")
    
    if total_time < 6 * 3600:  # 6å°æ—¶
        print(f"ğŸ‰ è®­ç»ƒåœ¨ç›®æ ‡æ—¶é—´å†…å®Œæˆï¼({total_time/3600:.1f}å°æ—¶ < 6å°æ—¶)")
    else:
        print(f"âš ï¸ è®­ç»ƒè¶…å‡ºç›®æ ‡æ—¶é—´ ({total_time/3600:.1f}å°æ—¶ > 6å°æ—¶)")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("1. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯")
    print("2. åˆ†æevaluationç»“æœ")
    print("3. å¦‚éœ€è¦å¯ä»¥è°ƒæ•´è¶…å‚æ•°é‡æ–°è®­ç»ƒ")


if __name__ == "__main__":
    main() 