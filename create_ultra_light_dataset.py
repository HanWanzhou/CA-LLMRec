#!/usr/bin/env python3
"""
CA-LLMRec è¶…è½»é‡åŒ–æ•°æ®é›†ç”Ÿæˆè„šæœ¬
å°†Amazon Movies and TVæ•°æ®é›†ç¼©å‡åˆ°åŸæ¥çš„5%ï¼ˆçº¦1000ç”¨æˆ·ï¼Œ3000ç‰©å“ï¼‰
ç›®æ ‡ï¼š6å°æ—¶å†…å®Œæˆå…¨éƒ¨è®­ç»ƒ
"""

import os
import sys
import json
import gzip
import random
from collections import defaultdict
from tqdm import tqdm
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')
sys.path.append('./pre_train/sasrec')

def parse(path):
    """è§£ægzipå‹ç¼©çš„JSONæ–‡ä»¶"""
    g = gzip.open(path, 'rb')
    for l in tqdm(g, desc="è§£ææ•°æ®"):
        yield json.loads(l)

def create_ultra_light_dataset(max_users=1000, max_items=3000, min_interactions=5):
    """
    åˆ›å»ºè¶…è½»é‡åŒ–æ•°æ®é›† - 5%æ•°æ®é‡
    Args:
        max_users: æœ€å¤§ç”¨æˆ·æ•°é‡ï¼ˆé»˜è®¤1000ï¼ŒåŸå§‹çº¦12ä¸‡ï¼‰
        max_items: æœ€å¤§ç‰©å“æ•°é‡ï¼ˆé»˜è®¤3000ï¼ŒåŸå§‹çº¦5ä¸‡ï¼‰
        min_interactions: æœ€å°äº¤äº’æ¬¡æ•°ï¼ˆä¿è¯æ•°æ®è´¨é‡ï¼‰
    """
    fname = 'Movies_and_TV'
    print(f"ğŸ”§ åˆ›å»ºè¶…è½»é‡åŒ–æ•°æ®é›†: {fname}")
    print(f"  ç›®æ ‡ç”¨æˆ·æ•°é‡: {max_users}")
    print(f"  ç›®æ ‡ç‰©å“æ•°é‡: {max_items}")
    print(f"  æœ€å°äº¤äº’æ¬¡æ•°: {min_interactions}")
    print(f"  é¢„è®¡æ•°æ®é‡: åŸå§‹çš„5%")
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_dir = './data/amazon'
    reviews_file = f'{data_dir}/{fname}.json.gz'
    reviews_file_5core = f'{data_dir}/{fname}_5.json.gz'
    meta_file = f'{data_dir}/meta_{fname}.json'
    meta_file_gz = f'{data_dir}/meta_{fname}.json.gz'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - ä¼˜å…ˆä½¿ç”¨5-coreç‰ˆæœ¬
    if os.path.exists(reviews_file_5core):
        reviews_file = reviews_file_5core
        print(f"âœ… ä½¿ç”¨5-coreæ•°æ®é›†: {reviews_file}")
    elif os.path.exists(reviews_file):
        print(f"âœ… ä½¿ç”¨å®Œæ•´æ•°æ®é›†: {reviews_file}")
    else:
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {reviews_file} æˆ– {reviews_file_5core}")
        print("è¯·å…ˆä¸‹è½½Amazon Movies and TVæ•°æ®é›†")
        return False
    
    # æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - æ”¯æŒ.json.gzæ ¼å¼
    if os.path.exists(meta_file_gz):
        meta_file = meta_file_gz
        print(f"âœ… ä½¿ç”¨å‹ç¼©å…ƒæ•°æ®: {meta_file}")
    elif os.path.exists(meta_file):
        print(f"âœ… ä½¿ç”¨å…ƒæ•°æ®: {meta_file}")
    else:
        print(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {meta_file} æˆ– {meta_file_gz}")
        print("è¯·å…ˆä¸‹è½½Amazon Movies and TVå…ƒæ•°æ®")
        return False
    
    # Step 1: ç»Ÿè®¡æ‰€æœ‰ç”¨æˆ·å’Œç‰©å“çš„äº¤äº’æ¬¡æ•°
    print("ğŸ“Š ç¬¬ä¸€éæ‰«æï¼šç»Ÿè®¡äº¤äº’æ¬¡æ•°...")
    countU = defaultdict(int)
    countP = defaultdict(int)
    
    for l in parse(reviews_file):
        # è¿‡æ»¤ä½è¯„åˆ†ï¼ˆå¯é€‰ï¼‰
        if l['overall'] < 3:
            continue
        asin = l['asin']
        rev = l['reviewerID']
        countU[rev] += 1
        countP[asin] += 1
    
    print(f"  åŸå§‹ç”¨æˆ·æ•°: {len(countU)}")
    print(f"  åŸå§‹ç‰©å“æ•°: {len(countP)}")
    
    # Step 2: é€‰æ‹©æœ€æ´»è·ƒçš„ç”¨æˆ·å’Œæœ€çƒ­é—¨çš„ç‰©å“
    print("ğŸ¯ é€‰æ‹©æ ¸å¿ƒç”¨æˆ·å’Œç‰©å“...")
    
    # æŒ‰äº¤äº’æ¬¡æ•°æ’åºï¼Œé€‰æ‹©æœ€æ´»è·ƒçš„ç”¨æˆ·
    sorted_users = sorted(countU.items(), key=lambda x: x[1], reverse=True)
    selected_users = set()
    for user, count in sorted_users:
        if len(selected_users) >= max_users:
            break
        if count >= min_interactions:
            selected_users.add(user)
    
    # æŒ‰äº¤äº’æ¬¡æ•°æ’åºï¼Œé€‰æ‹©æœ€çƒ­é—¨çš„ç‰©å“
    sorted_items = sorted(countP.items(), key=lambda x: x[1], reverse=True)
    selected_items = set()
    for item, count in sorted_items:
        if len(selected_items) >= max_items:
            break
        if count >= min_interactions:
            selected_items.add(item)
    
    print(f"  é€‰ä¸­ç”¨æˆ·: {len(selected_users)}")
    print(f"  é€‰ä¸­ç‰©å“: {len(selected_items)}")
    
    # Step 3: é‡æ–°æ‰«æï¼Œåªä¿ç•™é€‰ä¸­çš„ç”¨æˆ·-ç‰©å“äº¤äº’
    print("ğŸ”„ ç¬¬äºŒéæ‰«æï¼šæ„å»ºè½»é‡åŒ–æ•°æ®é›†...")
    
    usermap = {}
    usernum = 0
    itemmap = {}
    itemnum = 0
    User = {}
    review_dict = {}
    name_dict = {'title': {}, 'description': {}}
    
    # åŠ è½½å…ƒæ•°æ®
    print("ğŸ“š åŠ è½½ç‰©å“å…ƒæ•°æ®...")
    meta_dict = {}
    try:
        if meta_file.endswith('.gz'):
            # å¤„ç†gzipå‹ç¼©æ–‡ä»¶
            with gzip.open(meta_file, 'rt', encoding='utf-8') as f:
                for line in tqdm(f, desc="åŠ è½½å…ƒæ•°æ®"):
                    data = json.loads(line.strip())
                    if data['asin'] in selected_items:
                        meta_dict[data['asin']] = data
        else:
            # å¤„ç†æ™®é€šJSONæ–‡ä»¶
            with open(meta_file, 'r', encoding='utf-8') as f:
                for line in tqdm(f, desc="åŠ è½½å…ƒæ•°æ®"):
                    data = json.loads(line.strip())
                    if data['asin'] in selected_items:
                        meta_dict[data['asin']] = data
    except Exception as e:
        print(f"âš ï¸ å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
        meta_dict = {}
    
    # å¤„ç†è¯„è®ºæ•°æ®
    processed_interactions = 0
    for l in parse(reviews_file):
        asin = l['asin']
        rev = l['reviewerID']
        time = l['unixReviewTime']
        
        # åªå¤„ç†é€‰ä¸­çš„ç”¨æˆ·å’Œç‰©å“
        if rev not in selected_users or asin not in selected_items:
            continue
        
        # è¿‡æ»¤ä½è¯„åˆ†
        if l['overall'] < 3:
            continue
        
        # æ˜ å°„ç”¨æˆ·ID
        if rev not in usermap:
            usernum += 1
            usermap[rev] = usernum
            User[usernum] = []
        userid = usermap[rev]
        
        # æ˜ å°„ç‰©å“ID
        if asin not in itemmap:
            itemnum += 1
            itemmap[asin] = itemnum
        itemid = itemmap[asin]
        
        # æ·»åŠ äº¤äº’è®°å½•
        User[userid].append([itemid, time])
        processed_interactions += 1
        
        # ä¿å­˜è¯„è®ºä¿¡æ¯
        review_dict[(userid, itemid)] = {
            'rating': l['overall'],
            'review': l.get('reviewText', ''),
            'summary': l.get('summary', '')
        }
        
        # ä¿å­˜ç‰©å“ä¿¡æ¯
        if asin in meta_dict:
            meta_data = meta_dict[asin]
            name_dict['title'][itemid] = meta_data.get('title', 'No Title')
            # å¤„ç†descriptionå­—æ®µï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼ï¼‰
            description = meta_data.get('description', [])
            if isinstance(description, list):
                if len(description) == 0:
                    name_dict['description'][itemid] = 'Empty description'
                else:
                    name_dict['description'][itemid] = description[0]
            else:
                name_dict['description'][itemid] = description if description else 'Empty description'
        else:
            name_dict['title'][itemid] = 'No Title'
            name_dict['description'][itemid] = 'Empty description'
    
    print(f"  å¤„ç†äº¤äº’æ•°: {processed_interactions}")
    print(f"  æœ€ç»ˆç”¨æˆ·æ•°: {usernum}")
    print(f"  æœ€ç»ˆç‰©å“æ•°: {itemnum}")
    
    # Step 4: æŒ‰æ—¶é—´æ’åºç”¨æˆ·åºåˆ—
    print("â° æŒ‰æ—¶é—´æ’åºç”¨æˆ·åºåˆ—...")
    for userid in User:
        User[userid].sort(key=lambda x: x[1])  # æŒ‰æ—¶é—´æ’åº
        User[userid] = [x[0] for x in User[userid]]  # åªä¿ç•™ç‰©å“ID
    
    # Step 5: ä¿å­˜è½»é‡åŒ–æ•°æ®é›†
    output_suffix = f"_ultra_light_{max_users}u_{max_items}i"
    output_file = f"{data_dir}/{fname}{output_suffix}.txt"
    
    print(f"ğŸ’¾ ä¿å­˜è½»é‡åŒ–æ•°æ®é›†: {output_file}")
    
    # ä¿å­˜ç”¨æˆ·-ç‰©å“äº¤äº’å¯¹æ•°æ®ï¼ˆç¬¦åˆA-LLMRecæ ‡å‡†æ ¼å¼ï¼‰
    with open(output_file, 'w') as f:
        for user in range(1, usernum + 1):
            if user in User and len(User[user]) > 0:
                for item in User[user]:
                    f.write(f'{user} {item}\n')  # æ¯è¡Œä¸€ä¸ªç”¨æˆ·-ç‰©å“å¯¹
    
    # ä¿å­˜æ˜ å°„ä¿¡æ¯
    mapping_file = f"{data_dir}/{fname}{output_suffix}_mappings.json"
    mappings = {
        'usermap': usermap,
        'itemmap': itemmap,
        'user_count': usernum,
        'item_count': itemnum,
        'interaction_count': processed_interactions,
        'name_dict': name_dict,
        'stats': {
            'original_users': len(countU),
            'original_items': len(countP),
            'selected_users': len(selected_users),
            'selected_items': len(selected_items),
            'compression_ratio_users': len(selected_users) / len(countU),
            'compression_ratio_items': len(selected_items) / len(countP)
        }
    }
    
    with open(mapping_file, 'w') as f:
        json.dump(mappings, f, indent=2)
    
    # ä¿å­˜è¯„è®ºä¿¡æ¯ï¼ˆç”¨äºLLMè®­ç»ƒï¼‰
    review_file = f"{data_dir}/{fname}{output_suffix}_reviews.json"
    # è½¬æ¢å…ƒç»„é”®ä¸ºå­—ç¬¦ä¸²é”®ä»¥æ”¯æŒJSONåºåˆ—åŒ–
    review_dict_serializable = {f"{k[0]}_{k[1]}": v for k, v in review_dict.items()}
    with open(review_file, 'w') as f:
        json.dump(review_dict_serializable, f, indent=2)
    
    # ä¿å­˜ç‰©å“æ–‡æœ¬ä¿¡æ¯ï¼ˆç”¨äºA-LLMRecè®­ç»ƒï¼‰
    text_name_file = f"{data_dir}/{fname}{output_suffix}_text_name_dict.json.gz"
    import pickle
    with gzip.open(text_name_file, 'wb') as f:
        pickle.dump(name_dict, f)
    
    print("âœ… è¶…è½»é‡åŒ–æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  ç”¨æˆ·æ•°: {usernum} (å‹ç¼©ç‡: {len(selected_users)/len(countU):.1%})")
    print(f"  ç‰©å“æ•°: {itemnum} (å‹ç¼©ç‡: {len(selected_items)/len(countP):.1%})")
    print(f"  äº¤äº’æ•°: {processed_interactions}")
    print(f"  å¹³å‡æ¯ç”¨æˆ·äº¤äº’æ•°: {processed_interactions/usernum:.1f}")
    print(f"  æ•°æ®æ–‡ä»¶: {output_file}")
    print(f"  æ˜ å°„æ–‡ä»¶: {mapping_file}")
    print(f"  è¯„è®ºæ–‡ä»¶: {review_file}")
    print(f"  æ–‡æœ¬æ–‡ä»¶: {text_name_file}")
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åˆ›å»ºè¶…è½»é‡åŒ–Amazon Movies and TVæ•°æ®é›†')
    parser.add_argument('--max_users', type=int, default=1000, 
                       help='æœ€å¤§ç”¨æˆ·æ•°é‡ (é»˜è®¤: 1000)')
    parser.add_argument('--max_items', type=int, default=3000, 
                       help='æœ€å¤§ç‰©å“æ•°é‡ (é»˜è®¤: 3000)')
    parser.add_argument('--min_interactions', type=int, default=5, 
                       help='æœ€å°äº¤äº’æ¬¡æ•° (é»˜è®¤: 5)')
    
    args = parser.parse_args()
    
    print("ğŸš€ CA-LLMRec è¶…è½»é‡åŒ–æ•°æ®é›†ç”Ÿæˆ")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    os.makedirs('./data/amazon', exist_ok=True)
    
    # ç”Ÿæˆæ•°æ®é›†
    success = create_ultra_light_dataset(
        max_users=args.max_users,
        max_items=args.max_items,
        min_interactions=args.min_interactions
    )
    
    if success:
        print("\nğŸ‰ æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
        print(f"python train_ca_llmrec_ultra_light.py --max_users {args.max_users}")
    else:
        print("\nâŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥ï¼")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨")

if __name__ == "__main__":
    main() 