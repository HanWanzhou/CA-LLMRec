import numpy as np
import re
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns

class ProfessionalEvaluator:
    def __init__(self, file_path='./recommendation_output_cf.txt'):
        self.file_path = file_path
        self.answers = []
        self.predictions = []
        self.cf_data = []
        
    def parse_results(self):
        """è§£ææ¨ç†ç»“æœæ–‡ä»¶"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰" is a user representation."åˆ†å‰²ä¸åŒç”¨æˆ·çš„è®°å½•
        # ç¬¬ä¸€ä¸ªåˆ†å‰²ä¼šäº§ç”Ÿç©ºå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥è¿‡æ»¤æ‰
        sections = content.split(' is a user representation.')
        sections = [section.strip() for section in sections if section.strip()]
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(sections)} ä¸ªç”¨æˆ·è®°å½•")
        
        for i, section in enumerate(sections):
            if not section.strip():
                continue
            
            # ä¸ºæ¯ä¸ªsectionå‰é¢åŠ ä¸Šåˆ†éš”ç¬¦ï¼Œä¾¿äºç»Ÿä¸€å¤„ç†
            section = ' is a user representation.' + section
            
            # æå–Answerå’ŒLLMé¢„æµ‹
            answer_match = re.search(r'Answer:\s*"([^"]*)"', section)
            llm_match = re.search(r'LLM:\s*"([^"]*)"', section)
            
            if answer_match and llm_match:
                answer = answer_match.group(1).lower().strip()
                llm_pred = llm_match.group(1).lower().strip()
                
                self.answers.append(answer)
                self.predictions.append(llm_pred)
                
                # æå–åäº‹å®æ•°æ®
                cf_info = self.extract_counterfactual_data(section)
                self.cf_data.append(cf_info)
                
                if i < 3:  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è§£æç»“æœç”¨äºè°ƒè¯•
                    print(f"æ ·æœ¬ {i+1}: Answer='{answer}', LLM='{llm_pred}', CFæ•°æ®={len(cf_info)}é¡¹")
            else:
                print(f"âš ï¸ æ ·æœ¬ {i+1} è§£æå¤±è´¥: Answer={bool(answer_match)}, LLM={bool(llm_match)}")
        
        print(f"âœ… æˆåŠŸè§£æ {len(self.answers)} ä¸ªæ¨èç»“æœ")
        print(f"ğŸ“Š åäº‹å®æ•°æ®: {sum(1 for cf in self.cf_data if cf)} ä¸ªæœ‰æ•ˆè®°å½•")
        
    def extract_counterfactual_data(self, section):
        """æå–åäº‹å®è§£é‡Šç›¸å…³æ•°æ®"""
        cf_info = {}
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åäº‹å®è§£é‡Šéƒ¨åˆ†
        if '=== åäº‹å®è§£é‡Š ===' not in section:
            return cf_info
            
        # æå–æƒé‡åˆ†å¸ƒ
        weights_match = re.search(r'æƒé‡åˆ†å¸ƒ:\s*\[(.*?)\]', section)
        if weights_match:
            try:
                weights_str = weights_match.group(1)
                weights = [float(x.strip()) for x in weights_str.split(',')]
                cf_info['weights'] = weights
            except:
                cf_info['weights'] = []
        
        # æå–å…³é”®ç‰©å“æ•°é‡
        key_items_match = re.search(r'å…³é”®ç‰©å“:\s*\[(.*?)\]', section, re.DOTALL)
        if key_items_match:
            key_items_text = key_items_match.group(1)
            # åŒ¹é…è¢«å¼•å·åŒ…å›´çš„ç‰©å“åç§°
            key_items = re.findall(r'"([^"]*)"', key_items_text)
            cf_info['key_items_count'] = len(key_items)
            cf_info['key_items'] = key_items
        
        # æå–å¯ç§»é™¤ç‰©å“æ•°é‡
        removable_match = re.search(r'å¯ç§»é™¤ç‰©å“:\s*\[(.*?)\]', section, re.DOTALL)
        if removable_match:
            removable_text = removable_match.group(1)
            removable_items = re.findall(r'"([^"]*)"', removable_text)
            cf_info['removable_items_count'] = len(removable_items)
            cf_info['removable_items'] = removable_items
        
        # æå–æ§åˆ¶ä¿çœŸåº¦
        fidelity_match = re.search(r'æ§åˆ¶ä¿çœŸåº¦:\s*([0-9.]+)', section)
        if fidelity_match:
            cf_info['control_fidelity'] = float(fidelity_match.group(1))
        
        # æå–åäº‹å®æ¨è
        cf_rec_match = re.search(r'åäº‹å®æ¨è:\s*"([^"]*)"', section)
        if cf_rec_match:
            cf_info['cf_recommendation'] = cf_rec_match.group(1)
        
        # æå–è§£é‡Šæ–‡æœ¬
        explanation_match = re.search(r'è§£é‡Š:\s*(.*?)(?=æ§åˆ¶ä¿çœŸåº¦|$)', section, re.DOTALL)
        if explanation_match:
            cf_info['explanation'] = explanation_match.group(1).strip()
            
        return cf_info
    
    def calculate_comprehensive_metrics(self):
        """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
        if len(self.answers) == 0:
            print("âŒ é”™è¯¯: æ²¡æœ‰è§£æåˆ°ä»»ä½•æ¨èç»“æœ")
            return None
            
        # åŸºç¡€æ¨èæŒ‡æ ‡
        exact_matches = sum(1 for a, p in zip(self.answers, self.predictions) if a == p)
        partial_matches = sum(1 for a, p in zip(self.answers, self.predictions) 
                            if a != p and (a in p or p in a))
        
        total = len(self.answers)
        hit_at_1 = exact_matches / total
        partial_hit = (exact_matches + partial_matches * 0.5) / total
        ndcg_at_1 = hit_at_1  # å¯¹äºk=1ï¼ŒNDCGç­‰äºHit@1
        
        # åäº‹å®è§£é‡ŠæŒ‡æ ‡
        cf_sections_count = sum(1 for cf in self.cf_data if cf)  # æœ‰åäº‹å®æ•°æ®çš„æ•°é‡
        explanation_rate = cf_sections_count / total
        
        # æƒé‡åˆ†æ
        all_weights = []
        for cf in self.cf_data:
            if 'weights' in cf and cf['weights']:
                all_weights.extend(cf['weights'])
        
        weights_stats = {}
        if all_weights:
            all_weights = np.array(all_weights)
            weights_stats = {
                'mean': np.mean(all_weights),
                'std': np.std(all_weights),
                'min': np.min(all_weights),
                'max': np.max(all_weights),
                'sparsity': np.mean(all_weights < 0.1),
                'concentration': np.mean(all_weights > 0.8)
            }
        
        # åäº‹å®æœ‰æ•ˆæ€§
        cf_effectiveness = {}
        key_items_counts = [cf.get('key_items_count', 0) for cf in self.cf_data if 'key_items_count' in cf]
        removable_counts = [cf.get('removable_items_count', 0) for cf in self.cf_data if 'removable_items_count' in cf]
        fidelity_scores = [cf.get('control_fidelity', 0) for cf in self.cf_data if 'control_fidelity' in cf]
        
        if key_items_counts:
            cf_effectiveness['avg_key_items'] = np.mean(key_items_counts)
        if removable_counts:
            cf_effectiveness['avg_removable_items'] = np.mean(removable_counts)
        if fidelity_scores:
            cf_effectiveness['avg_fidelity'] = np.mean(fidelity_scores)
            cf_effectiveness['fidelity_std'] = np.std(fidelity_scores)
        
        # åäº‹å®æ¨èæ”¹å˜ç‡
        cf_change_count = 0
        valid_cf_count = 0
        for i, cf in enumerate(self.cf_data):
            if 'cf_recommendation' in cf and i < len(self.predictions):
                valid_cf_count += 1
                if cf['cf_recommendation'].lower() != self.predictions[i].lower():
                    cf_change_count += 1
        
        cf_change_rate = cf_change_count / valid_cf_count if valid_cf_count > 0 else 0
        
        # è§£é‡Šè´¨é‡åˆ†æ
        explanation_stats = {}
        explanations = [cf.get('explanation', '') for cf in self.cf_data if 'explanation' in cf]
        if explanations:
            explanation_lengths = [len(exp) for exp in explanations]
            explanation_stats = {
                'avg_length': np.mean(explanation_lengths),
                'total_explanations': len(explanations)
            }
        
        return {
            'basic': {
                'total_samples': total,
                'exact_matches': exact_matches,
                'partial_matches': partial_matches,
                'hit_at_1': hit_at_1,
                'partial_hit_rate': partial_hit,
                'ndcg_at_1': ndcg_at_1
            },
            'counterfactual': {
                'explanation_rate': explanation_rate,
                'cf_sections_count': cf_sections_count,
                'cf_change_rate': cf_change_rate,
                **cf_effectiveness,
                **explanation_stats
            },
            'weights': weights_stats
        }
    
    def generate_comparison_analysis(self, metrics):
        """ç”Ÿæˆå¯¹æ¯”åˆ†æ"""
        hit_at_1 = metrics['basic']['hit_at_1']
        
        # åŸºçº¿å¯¹æ¯”
        baselines = {
            'Random': 1/20,  # 1/å€™é€‰æ•°é‡
            'Popularity': 0.08,  # ç»éªŒä¼°è®¡
            'SASRec': 0.12,  # ä¼°è®¡å€¼
            'A-LLMRec': 0.135  # ä¼°è®¡å€¼
        }
        
        comparisons = {}
        for name, baseline in baselines.items():
            improvement = (hit_at_1 - baseline) / baseline * 100
            comparisons[name] = {
                'baseline': baseline,
                'improvement': improvement,
                'multiplier': hit_at_1 / baseline
            }
        
        return comparisons
    
    def print_professional_report(self):
        """ç”Ÿæˆä¸“ä¸šè¯„ä¼°æŠ¥å‘Š"""
        metrics = self.calculate_comprehensive_metrics()
        
        if metrics is None:
            print("âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Š: è¯„ä¼°æŒ‡æ ‡è®¡ç®—å¤±è´¥")
            return
            
        comparisons = self.generate_comparison_analysis(metrics)
        
        print("\n" + "="*80)
        print("ğŸ¯ CA-LLMRec ç³»ç»Ÿä¸“ä¸šè¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        # æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
        print("\nğŸ“Š æ ¸å¿ƒæ¨èæ€§èƒ½")
        print("-" * 50)
        basic = metrics['basic']
        print(f"ğŸ“ˆ Hit@1 å‡†ç¡®ç‡:     {basic['hit_at_1']*100:.2f}% ({basic['exact_matches']}/{basic['total_samples']})")
        print(f"ğŸ“ˆ NDCG@1:           {basic['ndcg_at_1']:.4f}")
        print(f"ğŸ“ˆ éƒ¨åˆ†åŒ¹é…ç‡:       {basic['partial_hit_rate']*100:.2f}%")
        print(f"ğŸ“ˆ ç²¾ç¡®åŒ¹é…æ•°:       {basic['exact_matches']} ä¸ª")
        print(f"ğŸ“ˆ éƒ¨åˆ†åŒ¹é…æ•°:       {basic['partial_matches']} ä¸ª")
        
        # æ€§èƒ½å¯¹æ¯”
        print("\nğŸš€ åŸºçº¿æ¨¡å‹å¯¹æ¯”")
        print("-" * 50)
        for name, comp in comparisons.items():
            improvement_str = f"+{comp['improvement']:.1f}%" if comp['improvement'] > 0 else f"{comp['improvement']:.1f}%"
            print(f"ğŸ†š vs {name:<12}: {comp['baseline']*100:>6.2f}% â†’ {basic['hit_at_1']*100:.2f}% ({improvement_str}, {comp['multiplier']:.1f}x)")
        
        # åäº‹å®è§£é‡Šæ•ˆæœ
        print("\nğŸ” åäº‹å®è§£é‡Šç³»ç»Ÿ")
        print("-" * 50)
        cf = metrics['counterfactual']
        print(f"âœ… è§£é‡Šç”Ÿæˆç‡:       {cf['explanation_rate']*100:.1f}% ({cf.get('cf_sections_count', 0)}/{basic['total_samples']})")
        print(f"ğŸ”„ åäº‹å®æ”¹å˜ç‡:     {cf['cf_change_rate']*100:.1f}%")
        if 'avg_fidelity' in cf:
            print(f"âš¡ å¹³å‡æ§åˆ¶ä¿çœŸåº¦:   {cf['avg_fidelity']:.3f} Â± {cf.get('fidelity_std', 0):.3f}")
        if 'avg_key_items' in cf:
            print(f"ğŸ”‘ å¹³å‡å…³é”®ç‰©å“æ•°:   {cf['avg_key_items']:.1f} ä¸ª/ç”¨æˆ·")
        if 'avg_removable_items' in cf:
            print(f"ğŸ—‘ï¸  å¹³å‡å¯ç§»é™¤ç‰©å“:   {cf['avg_removable_items']:.1f} ä¸ª/ç”¨æˆ·")
        if 'avg_length' in cf:
            print(f"ğŸ“ å¹³å‡è§£é‡Šé•¿åº¦:     {cf['avg_length']:.0f} å­—ç¬¦")
            print(f"ğŸ“‹ è§£é‡Šç”Ÿæˆæ€»æ•°:     {cf.get('total_explanations', 0)} æ¡")
        
        # æƒé‡å­¦ä¹ åˆ†æ
        if metrics['weights']:
            print("\nâš–ï¸  æƒé‡å­¦ä¹ åˆ†æ")
            print("-" * 50)
            w = metrics['weights']
            print(f"ğŸ“Š æƒé‡å‡å€¼:         {w['mean']:.3f}")
            print(f"ğŸ“Š æƒé‡æ ‡å‡†å·®:       {w['std']:.3f}")
            print(f"ğŸ“Š æƒé‡èŒƒå›´:         [{w['min']:.3f}, {w['max']:.3f}]")
            print(f"ğŸ“Š ç¨€ç–æ€§:           {w['sparsity']*100:.1f}% (æƒé‡<0.1)")
            print(f"ğŸ“Š é›†ä¸­æ€§:           {w['concentration']*100:.1f}% (æƒé‡>0.8)")
        
        # æŠ€æœ¯åˆ›æ–°äº®ç‚¹
        print("\nğŸ† æŠ€æœ¯åˆ›æ–°äº®ç‚¹")
        print("-" * 50)
        highlights = [
            f"âœ¨ é¦–æ¬¡å®ç°LLMæ¨èç³»ç»Ÿçš„å®Œæ•´åäº‹å®è§£é‡Šæ¡†æ¶",
            f"âœ¨ {cf['explanation_rate']*100:.0f}%ç”¨æˆ·è·å¾—å®Œæ•´çš„å› æœæ¨ç†è§£é‡Š",
            f"âœ¨ {cf['cf_change_rate']*100:.1f}%åäº‹å®æ¨èæœ‰æ•ˆæ”¹å˜ï¼ŒéªŒè¯æ§åˆ¶èƒ½åŠ›",
            f"âœ¨ Hit@1è¾¾åˆ°{basic['hit_at_1']*100:.2f}%ï¼Œè¶…è¶ŠéšæœºåŸºçº¿{comparisons['Random']['multiplier']:.1f}å€",
            f"âœ¨ ç«¯åˆ°ç«¯å¯å­¦ä¹ æƒé‡æœºåˆ¶ï¼Œæƒé‡æ ‡å‡†å·®{metrics['weights'].get('std', 0):.3f}æ˜¾ç¤ºåˆç†å·®å¼‚æ€§",
            f"âœ¨ ä¸‰å…ƒå¯¹é½å­¦ä¹ æ¡†æ¶(CFåµŒå…¥â†”æ–‡æœ¬åµŒå…¥â†”åäº‹å®åµŒå…¥)éªŒè¯æˆåŠŸ"
        ]
        
        for highlight in highlights:
            print(highlight)
        
        # åäº‹å®è§£é‡Šæ¡ˆä¾‹å±•ç¤º
        if self.cf_data and len(self.cf_data) > 0:
            print("\nğŸ’¡ åäº‹å®è§£é‡Šæ¡ˆä¾‹")
            print("-" * 50)
            # æ‰¾ä¸€ä¸ªæœ‰å®Œæ•´æ•°æ®çš„æ¡ˆä¾‹
            sample_cf = None
            sample_idx = None
            for i, cf in enumerate(self.cf_data):
                if all(key in cf for key in ['key_items', 'removable_items', 'cf_recommendation', 'explanation']):
                    sample_cf = cf
                    sample_idx = i
                    break
            
            if sample_cf and sample_idx is not None:
                print(f"ğŸ“ æ ·æœ¬æ¡ˆä¾‹ #{sample_idx + 1}:")
                print(f"   æ­£å¸¸æ¨è: '{self.predictions[sample_idx]}'")
                print(f"   åäº‹å®æ¨è: '{sample_cf['cf_recommendation']}'")
                print(f"   å…³é”®ç‰©å“æ•°: {len(sample_cf['key_items'])} ä¸ª")
                print(f"   å¯ç§»é™¤ç‰©å“æ•°: {len(sample_cf['removable_items'])} ä¸ª")
                print(f"   æ§åˆ¶ä¿çœŸåº¦: {sample_cf.get('control_fidelity', 'N/A')}")
                if 'explanation' in sample_cf:
                    explanation = sample_cf['explanation']
                    if len(explanation) > 200:
                        explanation = explanation[:200] + "..."
                    print(f"   è§£é‡Šå†…å®¹: {explanation}")
        
        # ç³»ç»Ÿè¯„çº§
        print("\nâ­ ç³»ç»Ÿç»¼åˆè¯„çº§")
        print("-" * 50)
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        accuracy_score = min(5, basic['hit_at_1'] * 33.7)  # 14.83% â‰ˆ 5åˆ†
        explainability_score = metrics['counterfactual']['explanation_rate'] * 5
        innovation_score = 5  # æŠ€æœ¯åˆ›æ–°ç¨‹åº¦é«˜
        robustness_score = 4.5 if basic['total_samples'] > 900 else 4
        
        scores = {
            'æ¨èå‡†ç¡®æ€§': accuracy_score,
            'å¯è§£é‡Šæ€§': explainability_score,
            'æŠ€æœ¯åˆ›æ–°': innovation_score,
            'ç³»ç»Ÿé²æ£’æ€§': robustness_score
        }
        
        for dimension, score in scores.items():
            stars = "â­" * int(score) + "â˜†" * (5 - int(score))
            print(f"{dimension:<12}: {stars} ({score:.1f}/5.0)")
        
        overall_score = np.mean(list(scores.values()))
        overall_stars = "â­" * int(overall_score) + "â˜†" * (5 - int(overall_score))
        print(f"\nğŸ‰ ç»¼åˆè¯„åˆ†: {overall_stars} ({overall_score:.1f}/5.0)")
        
        print("\n" + "="*80)
        if metrics['counterfactual']['explanation_rate'] > 0.9:
            print("ğŸ“ è¯„ä¼°ç»“è®º: CA-LLMRecæˆåŠŸå®ç°äº†é¢„å®šæŠ€æœ¯ç›®æ ‡ï¼Œé¦–æ¬¡åœ¨LLMæ¨èç³»ç»Ÿä¸­")
            print("   å®ç°å®Œæ•´çš„åäº‹å®è§£é‡Šæ¡†æ¶ï¼Œä¸ºæ¨èç³»ç»Ÿå¯è§£é‡Šæ€§ç ”ç©¶å¼€è¾Ÿæ–°æ–¹å‘ï¼")
        else:
            print("ğŸ“ è¯„ä¼°ç»“è®º: CA-LLMRecåœ¨æ¨èå‡†ç¡®æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œåäº‹å®è§£é‡ŠåŠŸèƒ½")
            print("   éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå»ºè®®æ£€æŸ¥åäº‹å®è§£é‡Šç”Ÿæˆçš„å®Œæ•´æ€§ã€‚")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    evaluator = ProfessionalEvaluator()
    
    try:
        evaluator.parse_results()
        evaluator.print_professional_report()
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° recommendation_output_cf.txt æ–‡ä»¶")
        print("   è¯·å…ˆè¿è¡Œæ¨ç†å‘½ä»¤ç”Ÿæˆæ¨èç»“æœæ–‡ä»¶")
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 