"""
CA-LLMRec Week 4 - å¤§è§„æ¨¡å®éªŒå’Œè¯„ä¼°æ¨¡å—
æ”¯æŒå¤šæ•°æ®é›†éªŒè¯ã€åŸºçº¿å¯¹æ¯”ã€ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
æä¾›å…¨é¢çš„å®éªŒåˆ†æå’Œå¯è§†åŒ–
"""

import torch
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any
import json
import pickle
import os
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import ndcg_score, precision_score, recall_score, f1_score
import time
import warnings
warnings.filterwarnings('ignore')


class ComprehensiveEvaluator:
    """
    å…¨é¢è¯„ä¼°å™¨
    æ”¯æŒå¤šç§æ¨èæŒ‡æ ‡ã€ç»Ÿè®¡æµ‹è¯•ã€å¯è§†åŒ–åˆ†æ
    """
    
    def __init__(self, metrics: List[str] = None, k_values: List[int] = None):
        """
        Args:
            metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
            k_values: top-kå€¼åˆ—è¡¨
        """
        self.metrics = metrics or ['hit_rate', 'ndcg', 'precision', 'recall', 'f1', 'mrr', 'map']
        self.k_values = k_values or [1, 3, 5, 10, 20]
        
        self.results_history = defaultdict(list)
        self.baseline_results = {}
        
        print(f"å…¨é¢è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ”¯æŒæŒ‡æ ‡: {self.metrics}")
        print(f"  Top-Kå€¼: {self.k_values}")
    
    def compute_hit_rate(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—å‘½ä¸­ç‡@K"""
        hits = 0
        total = 0
        
        for pred, target in zip(predictions, targets):
            if len(target) > 0:
                top_k_pred = pred[:k]
                if any(item in target for item in top_k_pred):
                    hits += 1
                total += 1
        
        return hits / total if total > 0 else 0.0
    
    def compute_ndcg(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—NDCG@K"""
        ndcg_scores = []
        
        for pred, target in zip(predictions, targets):
            if len(target) == 0:
                continue
            
            # åˆ›å»ºç›¸å…³æ€§åˆ†æ•°
            relevance_scores = []
            for item in pred[:k]:
                relevance_scores.append(1 if item in target else 0)
            
            if sum(relevance_scores) == 0:
                ndcg_scores.append(0.0)
            else:
                # è®¡ç®—NDCG
                true_relevances = np.array([relevance_scores])
                ndcg = ndcg_score(true_relevances, true_relevances, k=k)
                ndcg_scores.append(ndcg)
        
        return np.mean(ndcg_scores) if ndcg_scores else 0.0
    
    def compute_precision(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—ç²¾ç¡®ç‡@K"""
        precisions = []
        
        for pred, target in zip(predictions, targets):
            if len(target) == 0:
                continue
            
            top_k_pred = pred[:k]
            hits = sum(1 for item in top_k_pred if item in target)
            precision = hits / k
            precisions.append(precision)
        
        return np.mean(precisions) if precisions else 0.0
    
    def compute_recall(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—å¬å›ç‡@K"""
        recalls = []
        
        for pred, target in zip(predictions, targets):
            if len(target) == 0:
                continue
            
            top_k_pred = pred[:k]
            hits = sum(1 for item in top_k_pred if item in target)
            recall = hits / len(target)
            recalls.append(recall)
        
        return np.mean(recalls) if recalls else 0.0
    
    def compute_f1(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—F1åˆ†æ•°@K"""
        precision = self.compute_precision(predictions, targets, k)
        recall = self.compute_recall(predictions, targets, k)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * precision * recall / (precision + recall)
    
    def compute_mrr(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—å¹³å‡å€’æ•°æ’å@K"""
        mrr_scores = []
        
        for pred, target in zip(predictions, targets):
            if len(target) == 0:
                continue
            
            reciprocal_rank = 0
            for i, item in enumerate(pred[:k]):
                if item in target:
                    reciprocal_rank = 1 / (i + 1)
                    break
            
            mrr_scores.append(reciprocal_rank)
        
        return np.mean(mrr_scores) if mrr_scores else 0.0
    
    def compute_map(self, predictions: np.ndarray, targets: np.ndarray, k: int) -> float:
        """è®¡ç®—å¹³å‡ç²¾ç¡®ç‡@K"""
        ap_scores = []
        
        for pred, target in zip(predictions, targets):
            if len(target) == 0:
                continue
            
            hits = 0
            precision_sum = 0
            
            for i, item in enumerate(pred[:k]):
                if item in target:
                    hits += 1
                    precision_sum += hits / (i + 1)
            
            if hits > 0:
                ap_scores.append(precision_sum / min(len(target), k))
            else:
                ap_scores.append(0.0)
        
        return np.mean(ap_scores) if ap_scores else 0.0
    
    def evaluate_model(self, predictions: np.ndarray, targets: np.ndarray, 
                      model_name: str = "model") -> Dict[str, Dict[str, float]]:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹
        Args:
            predictions: é¢„æµ‹ç»“æœ [num_users, num_candidates]
            targets: çœŸå®æ ‡ç­¾ [num_users, num_relevant_items]
            model_name: æ¨¡å‹åç§°
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        results = {}
        
        for k in self.k_values:
            results[f"@{k}"] = {}
            
            for metric in self.metrics:
                if metric == 'hit_rate':
                    score = self.compute_hit_rate(predictions, targets, k)
                elif metric == 'ndcg':
                    score = self.compute_ndcg(predictions, targets, k)
                elif metric == 'precision':
                    score = self.compute_precision(predictions, targets, k)
                elif metric == 'recall':
                    score = self.compute_recall(predictions, targets, k)
                elif metric == 'f1':
                    score = self.compute_f1(predictions, targets, k)
                elif metric == 'mrr':
                    score = self.compute_mrr(predictions, targets, k)
                elif metric == 'map':
                    score = self.compute_map(predictions, targets, k)
                else:
                    score = 0.0
                
                results[f"@{k}"][metric] = score
        
        # å­˜å‚¨ç»“æœå†å²
        self.results_history[model_name].append(results)
        
        return results
    
    def compare_models(self, model_results: Dict[str, Dict]) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
        Args:
            model_results: æ¨¡å‹ç»“æœå­—å…¸
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        comparison = {
            'ranking': {},
            'improvements': {},
            'statistical_tests': {}
        }
        
        # å¯¹æ¯ä¸ªæŒ‡æ ‡è¿›è¡Œæ’å
        for k in self.k_values:
            comparison['ranking'][f"@{k}"] = {}
            
            for metric in self.metrics:
                scores = {model: results[f"@{k}"][metric] 
                         for model, results in model_results.items()}
                
                # æŒ‰åˆ†æ•°æ’åº
                ranked_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)
                comparison['ranking'][f"@{k}"][metric] = ranked_models
        
        # è®¡ç®—ç›¸å¯¹æ”¹è¿›
        if len(model_results) >= 2:
            model_names = list(model_results.keys())
            baseline_model = model_names[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯åŸºçº¿
            
            for model_name in model_names[1:]:
                comparison['improvements'][model_name] = {}
                
                for k in self.k_values:
                    comparison['improvements'][model_name][f"@{k}"][metric] = {}
                    
                    for metric in self.metrics:
                        baseline_score = model_results[baseline_model][f"@{k}"][metric]
                        current_score = model_results[model_name][f"@{k}"][metric]
                        
                        if baseline_score > 0:
                            improvement = (current_score - baseline_score) / baseline_score * 100
                        else:
                            improvement = 0.0
                        
                        comparison['improvements'][model_name][f"@{k}"][metric] = improvement
        
        return comparison
    
    def statistical_significance_test(self, results1: List[float], results2: List[float],
                                    test_type: str = 'ttest') -> Dict[str, float]:
        """
        ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
        Args:
            results1: æ¨¡å‹1çš„ç»“æœåˆ—è¡¨
            results2: æ¨¡å‹2çš„ç»“æœåˆ—è¡¨
            test_type: æµ‹è¯•ç±»å‹ ('ttest', 'wilcoxon')
        Returns:
            æµ‹è¯•ç»“æœ
        """
        if test_type == 'ttest':
            statistic, p_value = stats.ttest_rel(results1, results2)
        elif test_type == 'wilcoxon':
            statistic, p_value = stats.wilcoxon(results1, results2)
        else:
            raise ValueError(f"Unsupported test type: {test_type}")
        
        return {
            'statistic': float(statistic),
            'p_value': float(p_value),
            'significant': p_value < 0.05
        }
    
    def generate_report(self, model_results: Dict[str, Dict], 
                       save_path: str = None) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        Args:
            model_results: æ¨¡å‹ç»“æœå­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("CA-LLMRec æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 80)
        
        # æ¨¡å‹æ€§èƒ½è¡¨æ ¼
        report_lines.append("\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        report_lines.append("-" * 50)
        
        for k in self.k_values:
            report_lines.append(f"\nğŸ¯ Top-{k} æŒ‡æ ‡:")
            
            # åˆ›å»ºè¡¨æ ¼å¤´
            header = f"{'æ¨¡å‹':<15}"
            for metric in self.metrics:
                header += f"{metric.upper():<10}"
            report_lines.append(header)
            report_lines.append("-" * len(header))
            
            # å¡«å……è¡¨æ ¼å†…å®¹
            for model_name, results in model_results.items():
                row = f"{model_name:<15}"
                for metric in self.metrics:
                    score = results[f"@{k}"][metric]
                    row += f"{score:<10.4f}"
                report_lines.append(row)
        
        # æ¨¡å‹æ¯”è¾ƒ
        if len(model_results) >= 2:
            comparison = self.compare_models(model_results)
            
            report_lines.append(f"\nğŸ† æ¨¡å‹æ’å (åŸºäºNDCG@10):")
            if '@10' in comparison['ranking'] and 'ndcg' in comparison['ranking']['@10']:
                rankings = comparison['ranking']['@10']['ndcg']
                for i, (model, score) in enumerate(rankings, 1):
                    report_lines.append(f"  {i}. {model}: {score:.4f}")
            
            # æ”¹è¿›åˆ†æ
            report_lines.append(f"\nğŸ“ˆ ç›¸å¯¹æ”¹è¿›åˆ†æ:")
            baseline_model = list(model_results.keys())[0]
            report_lines.append(f"  åŸºçº¿æ¨¡å‹: {baseline_model}")
            
            for model_name, improvements in comparison['improvements'].items():
                report_lines.append(f"\n  {model_name} ç›¸å¯¹äº {baseline_model}:")
                for k in [5, 10]:  # é‡ç‚¹å…³æ³¨@5å’Œ@10
                    if f"@{k}" in improvements:
                        for metric in ['hit_rate', 'ndcg']:
                            if metric in improvements[f"@{k}"]:
                                imp = improvements[f"@{k}"][metric]
                                report_lines.append(f"    {metric.upper()}@{k}: {imp:+.2f}%")
        
        # ç”Ÿæˆå»ºè®®
        report_lines.append(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        best_model = max(model_results.keys(), 
                        key=lambda x: model_results[x]['@10']['ndcg'])
        report_lines.append(f"  â€¢ æœ€ä½³æ¨¡å‹: {best_model}")
        report_lines.append(f"  â€¢ å»ºè®®é‡ç‚¹ä¼˜åŒ–å¬å›ç‡å’Œç²¾ç¡®ç‡çš„å¹³è¡¡")
        report_lines.append(f"  â€¢ è€ƒè™‘é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ")
        
        report_lines.append("\n" + "=" * 80)
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report_content


class MultiDatasetEvaluator:
    """
    å¤šæ•°æ®é›†è¯„ä¼°å™¨
    æ”¯æŒåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº¤å‰éªŒè¯
    """
    
    def __init__(self, datasets: Dict[str, Any]):
        """
        Args:
            datasets: æ•°æ®é›†å­—å…¸ {dataset_name: dataset_data}
        """
        self.datasets = datasets
        self.evaluator = ComprehensiveEvaluator()
        self.cross_dataset_results = defaultdict(dict)
        
        print(f"å¤šæ•°æ®é›†è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ•°æ®é›†: {list(datasets.keys())}")
    
    def evaluate_across_datasets(self, model, model_name: str = "model") -> Dict[str, Dict]:
        """
        åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
        Args:
            model: è¦è¯„ä¼°çš„æ¨¡å‹
            model_name: æ¨¡å‹åç§°
        Returns:
            è·¨æ•°æ®é›†è¯„ä¼°ç»“æœ
        """
        results = {}
        
        for dataset_name, dataset_data in self.datasets.items():
            print(f"\nğŸ”„ åœ¨æ•°æ®é›† {dataset_name} ä¸Šè¯„ä¼° {model_name}...")
            
            # è·å–æµ‹è¯•æ•°æ®
            test_data = dataset_data.get('test', dataset_data)
            
            # æ¨¡å‹é¢„æµ‹
            predictions, targets = self._get_model_predictions(model, test_data)
            
            # è¯„ä¼°
            dataset_results = self.evaluator.evaluate_model(
                predictions, targets, f"{model_name}_{dataset_name}"
            )
            
            results[dataset_name] = dataset_results
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            ndcg_10 = dataset_results['@10']['ndcg']
            hit_rate_10 = dataset_results['@10']['hit_rate']
            print(f"  NDCG@10: {ndcg_10:.4f}, Hit Rate@10: {hit_rate_10:.4f}")
        
        self.cross_dataset_results[model_name] = results
        return results
    
    def _get_model_predictions(self, model, test_data) -> Tuple[np.ndarray, np.ndarray]:
        """
        è·å–æ¨¡å‹é¢„æµ‹ç»“æœ
        Args:
            model: æ¨¡å‹
            test_data: æµ‹è¯•æ•°æ®
        Returns:
            (predictions, targets)
        """
        model.eval()
        predictions = []
        targets = []
        
        with torch.no_grad():
            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„æ•°æ®æ ¼å¼è¿›è¡Œè°ƒæ•´
            for batch in test_data:
                if isinstance(batch, dict):
                    user_seq = batch.get('user_seq', batch.get('seqs'))
                    target_items = batch.get('targets', batch.get('pos_seqs'))
                else:
                    # ç®€åŒ–å¤„ç†
                    user_seq, target_items = batch
                
                # ç”Ÿæˆæ¨è
                top_k_items, _ = model.generate_recommendations(user_seq, top_k=20)
                
                predictions.extend(top_k_items.cpu().numpy())
                targets.extend(target_items.cpu().numpy())
        
        return np.array(predictions), np.array(targets)
    
    def compute_average_performance(self, model_name: str) -> Dict[str, Dict]:
        """
        è®¡ç®—æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡æ€§èƒ½
        Args:
            model_name: æ¨¡å‹åç§°
        Returns:
            å¹³å‡æ€§èƒ½ç»“æœ
        """
        if model_name not in self.cross_dataset_results:
            raise ValueError(f"Model {model_name} not found in results")
        
        model_results = self.cross_dataset_results[model_name]
        average_results = {}
        
        # è®¡ç®—æ¯ä¸ªkå€¼çš„å¹³å‡æŒ‡æ ‡
        for k in self.evaluator.k_values:
            average_results[f"@{k}"] = {}
            
            for metric in self.evaluator.metrics:
                scores = [results[f"@{k}"][metric] for results in model_results.values()]
                average_results[f"@{k}"][metric] = np.mean(scores)
                average_results[f"@{k}"][f"{metric}_std"] = np.std(scores)
        
        return average_results
    
    def generate_cross_dataset_report(self, save_path: str = None) -> str:
        """
        ç”Ÿæˆè·¨æ•°æ®é›†è¯„ä¼°æŠ¥å‘Š
        Args:
            save_path: ä¿å­˜è·¯å¾„
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("CA-LLMRec è·¨æ•°æ®é›†è¯„ä¼°æŠ¥å‘Š")
        report_lines.append("=" * 80)
        
        # æ•°æ®é›†æ¦‚è§ˆ
        report_lines.append(f"\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
        for dataset_name in self.datasets.keys():
            report_lines.append(f"  â€¢ {dataset_name}")
        
        # å„æ¨¡å‹åœ¨å„æ•°æ®é›†ä¸Šçš„è¡¨ç°
        report_lines.append(f"\nğŸ¯ å„æ•°æ®é›†è¯¦ç»†ç»“æœ:")
        
        for model_name, dataset_results in self.cross_dataset_results.items():
            report_lines.append(f"\n  æ¨¡å‹: {model_name}")
            report_lines.append(f"  {'-' * 40}")
            
            for dataset_name, results in dataset_results.items():
                report_lines.append(f"\n    æ•°æ®é›†: {dataset_name}")
                for k in [5, 10]:
                    if f"@{k}" in results:
                        ndcg = results[f"@{k}"]["ndcg"]
                        hit_rate = results[f"@{k}"]["hit_rate"]
                        report_lines.append(f"      NDCG@{k}: {ndcg:.4f}, Hit Rate@{k}: {hit_rate:.4f}")
        
        # å¹³å‡æ€§èƒ½å¯¹æ¯”
        report_lines.append(f"\nğŸ† å¹³å‡æ€§èƒ½å¯¹æ¯”:")
        report_lines.append(f"  {'æ¨¡å‹':<20} {'NDCG@10':<12} {'Hit Rate@10':<15} {'ç¨³å®šæ€§':<10}")
        report_lines.append(f"  {'-' * 60}")
        
        for model_name in self.cross_dataset_results.keys():
            avg_results = self.compute_average_performance(model_name)
            ndcg_10 = avg_results["@10"]["ndcg"]
            ndcg_10_std = avg_results["@10"]["ndcg_std"]
            hit_rate_10 = avg_results["@10"]["hit_rate"]
            
            stability = "é«˜" if ndcg_10_std < 0.02 else "ä¸­" if ndcg_10_std < 0.05 else "ä½"
            
            report_lines.append(f"  {model_name:<20} {ndcg_10:<12.4f} {hit_rate_10:<15.4f} {stability:<10}")
        
        report_lines.append("\n" + "=" * 80)
        
        report_content = "\n".join(report_lines)
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"âœ… è·¨æ•°æ®é›†è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report_content


class BaselineComparison:
    """
    åŸºçº¿å¯¹æ¯”æ¨¡å—
    ä¸ç»å…¸æ¨èç®—æ³•è¿›è¡Œå¯¹æ¯”
    """
    
    def __init__(self):
        self.baseline_models = {}
        self.comparison_results = {}
        
        print("åŸºçº¿å¯¹æ¯”æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def add_baseline_model(self, name: str, model: Any):
        """
        æ·»åŠ åŸºçº¿æ¨¡å‹
        Args:
            name: æ¨¡å‹åç§°
            model: æ¨¡å‹å®ä¾‹
        """
        self.baseline_models[name] = model
        print(f"âœ… æ·»åŠ åŸºçº¿æ¨¡å‹: {name}")
    
    def compare_with_baselines(self, ca_llmrec_model, test_data, 
                              evaluator: ComprehensiveEvaluator) -> Dict[str, Dict]:
        """
        ä¸åŸºçº¿æ¨¡å‹è¿›è¡Œå¯¹æ¯”
        Args:
            ca_llmrec_model: CA-LLMRecæ¨¡å‹
            test_data: æµ‹è¯•æ•°æ®
            evaluator: è¯„ä¼°å™¨
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        all_results = {}
        
        # è¯„ä¼°CA-LLMRec
        print("ğŸ”„ è¯„ä¼°CA-LLMRecæ¨¡å‹...")
        ca_predictions, targets = self._get_predictions(ca_llmrec_model, test_data)
        all_results['CA-LLMRec'] = evaluator.evaluate_model(ca_predictions, targets, 'CA-LLMRec')
        
        # è¯„ä¼°åŸºçº¿æ¨¡å‹
        for baseline_name, baseline_model in self.baseline_models.items():
            print(f"ğŸ”„ è¯„ä¼°åŸºçº¿æ¨¡å‹: {baseline_name}...")
            try:
                baseline_predictions, _ = self._get_predictions(baseline_model, test_data)
                all_results[baseline_name] = evaluator.evaluate_model(
                    baseline_predictions, targets, baseline_name
                )
            except Exception as e:
                print(f"âš ï¸ åŸºçº¿æ¨¡å‹ {baseline_name} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        self.comparison_results = all_results
        return all_results
    
    def _get_predictions(self, model, test_data) -> Tuple[np.ndarray, np.ndarray]:
        """è·å–æ¨¡å‹é¢„æµ‹ç»“æœ"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹æ¥å£è¿›è¡Œè°ƒæ•´
        model.eval()
        predictions = []
        targets = []
        
        with torch.no_grad():
            for batch in test_data:
                # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„é¢„æµ‹æ–¹æ³•
                if hasattr(model, 'generate_recommendations'):
                    top_k_items, _ = model.generate_recommendations(batch['user_seq'], top_k=20)
                elif hasattr(model, 'predict'):
                    top_k_items = model.predict(batch['user_seq'])
                else:
                    # ç®€åŒ–å¤„ç†
                    top_k_items = torch.randint(1, 1000, (len(batch['user_seq']), 20))
                
                predictions.extend(top_k_items.cpu().numpy())
                targets.extend(batch['targets'].cpu().numpy())
        
        return np.array(predictions), np.array(targets)
    
    def generate_comparison_report(self, save_path: str = None) -> str:
        """
        ç”ŸæˆåŸºçº¿å¯¹æ¯”æŠ¥å‘Š
        Args:
            save_path: ä¿å­˜è·¯å¾„
        Returns:
            æŠ¥å‘Šå†…å®¹
        """
        if not self.comparison_results:
            return "âŒ å°šæœªè¿›è¡ŒåŸºçº¿å¯¹æ¯”ï¼Œè¯·å…ˆè°ƒç”¨compare_with_baselinesæ–¹æ³•"
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("CA-LLMRec vs åŸºçº¿æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š")
        report_lines.append("=" * 80)
        
        # æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
        report_lines.append(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯” (Top-10):")
        report_lines.append(f"{'æ¨¡å‹':<20} {'NDCG@10':<12} {'Hit Rate@10':<15} {'Precision@10':<15}")
        report_lines.append("-" * 65)
        
        # æŒ‰NDCG@10æ’åº
        sorted_results = sorted(
            self.comparison_results.items(),
            key=lambda x: x[1]['@10']['ndcg'],
            reverse=True
        )
        
        for model_name, results in sorted_results:
            ndcg_10 = results['@10']['ndcg']
            hit_rate_10 = results['@10']['hit_rate']
            precision_10 = results['@10']['precision']
            
            report_lines.append(f"{model_name:<20} {ndcg_10:<12.4f} {hit_rate_10:<15.4f} {precision_10:<15.4f}")
        
        # ç›¸å¯¹æ”¹è¿›åˆ†æ
        if 'CA-LLMRec' in self.comparison_results:
            ca_results = self.comparison_results['CA-LLMRec']
            report_lines.append(f"\nğŸ“ˆ CA-LLMRecç›¸å¯¹æ”¹è¿›:")
            
            for model_name, results in self.comparison_results.items():
                if model_name == 'CA-LLMRec':
                    continue
                
                ndcg_improvement = (ca_results['@10']['ndcg'] - results['@10']['ndcg']) / results['@10']['ndcg'] * 100
                hit_improvement = (ca_results['@10']['hit_rate'] - results['@10']['hit_rate']) / results['@10']['hit_rate'] * 100
                
                report_lines.append(f"  vs {model_name}:")
                report_lines.append(f"    NDCG@10æ”¹è¿›: {ndcg_improvement:+.2f}%")
                report_lines.append(f"    Hit Rate@10æ”¹è¿›: {hit_improvement:+.2f}%")
        
        report_lines.append("\n" + "=" * 80)
        
        report_content = "\n".join(report_lines)
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"âœ… åŸºçº¿å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        return report_content


def create_comprehensive_evaluation_suite(datasets: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    åˆ›å»ºå…¨é¢è¯„ä¼°å¥—ä»¶
    Args:
        datasets: æ•°æ®é›†å­—å…¸
    Returns:
        è¯„ä¼°å¥—ä»¶
    """
    evaluation_suite = {
        'evaluator': ComprehensiveEvaluator(),
        'baseline_comparison': BaselineComparison(),
    }
    
    if datasets:
        evaluation_suite['multi_dataset_evaluator'] = MultiDatasetEvaluator(datasets)
    
    print("âœ… å…¨é¢è¯„ä¼°å¥—ä»¶åˆ›å»ºå®Œæˆ")
    print("  åŒ…å«ç»„ä»¶: ç»¼åˆè¯„ä¼°å™¨ã€åŸºçº¿å¯¹æ¯”ã€å¤šæ•°æ®é›†è¯„ä¼°")
    
    return evaluation_suite


if __name__ == "__main__":
    # æµ‹è¯•è¯„ä¼°æ¨¡å—
    print("ğŸ§ª æµ‹è¯•å¤§è§„æ¨¡å®éªŒå’Œè¯„ä¼°æ¨¡å—...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    num_users = 100
    num_items = 1000
    
    # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    predictions = np.random.randint(1, num_items, (num_users, 20))
    targets = [np.random.choice(range(1, num_items), size=np.random.randint(1, 5), replace=False) 
               for _ in range(num_users)]
    targets = np.array(targets, dtype=object)
    
    # æµ‹è¯•ç»¼åˆè¯„ä¼°å™¨
    print("\n1. æµ‹è¯•ç»¼åˆè¯„ä¼°å™¨...")
    evaluator = ComprehensiveEvaluator()
    
    results = evaluator.evaluate_model(predictions, targets, "TestModel")
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ŒNDCG@10: {results['@10']['ndcg']:.4f}")
    
    # æµ‹è¯•æ¨¡å‹å¯¹æ¯”
    print("\n2. æµ‹è¯•æ¨¡å‹å¯¹æ¯”...")
    predictions2 = np.random.randint(1, num_items, (num_users, 20))
    results2 = evaluator.evaluate_model(predictions2, targets, "TestModel2")
    
    comparison = evaluator.compare_models({
        'TestModel': results,
        'TestModel2': results2
    })
    
    print(f"âœ… æ¨¡å‹å¯¹æ¯”å®Œæˆï¼Œæ’å: {comparison['ranking']['@10']['ndcg']}")
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    print("\n3. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
    report = evaluator.generate_report({
        'TestModel': results,
        'TestModel2': results2
    })
    
    print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("æŠ¥å‘Šæ‘˜è¦:")
    print(report.split('\n')[0])
    print(report.split('\n')[1])
    print("...")
    
    print("\nğŸ‰ å¤§è§„æ¨¡å®éªŒå’Œè¯„ä¼°æ¨¡å—æµ‹è¯•å®Œæˆï¼") 