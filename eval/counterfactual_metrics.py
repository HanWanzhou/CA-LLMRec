"""
CA-LLMRec åäº‹å®è¯„ä¼°æŒ‡æ ‡æ¨¡å— - Week 2
åŸºäºUCRé¡¹ç›®çš„è¯„ä¼°æ–¹æ³•ï¼Œé’ˆå¯¹åäº‹å®æ¨èç³»ç»Ÿæ‰©å±•
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score
import json


class CounterfactualMetrics:
    """
    åäº‹å®è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨ - Week 2
    åŸºäºUCRçš„è¯„ä¼°æ¡†æ¶ï¼Œæ‰©å±•æ”¯æŒåäº‹å®æ¨èè¯„ä¼°
    """
    def __init__(self, device: str = 'cuda'):
        self.device = device
        self.reset()
        
    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        # åŸºç¡€æ¨èæŒ‡æ ‡
        self.ndcg_scores = []
        self.hit_scores = []
        self.precision_scores = []
        self.recall_scores = []
        
        # åäº‹å®ç‰¹æœ‰æŒ‡æ ‡
        self.cf_ndcg_scores = []
        self.cf_hit_scores = []
        self.alignment_losses = []
        self.sparsity_scores = []
        
        # å¯¹æ¯”æŒ‡æ ‡
        self.improvement_scores = []
        self.fidelity_scores = []
        self.diversity_scores = []
        
        # è§£é‡Šè´¨é‡æŒ‡æ ‡
        self.explanation_lengths = []
        self.explanation_coverage = []
        
    def compute_ndcg_at_k(self, predictions: torch.Tensor, targets: torch.Tensor, k: int = 10) -> float:
        """
        è®¡ç®—NDCG@K - åŸºäºUCRçš„è¯„ä¼°æ–¹æ³•
        Args:
            predictions: é¢„æµ‹åˆ†æ•° [batch_size, num_items]
            targets: çœŸå®æ ‡ç­¾ [batch_size, num_items]
            k: top-k
        Returns:
            NDCG@Kåˆ†æ•°
        """
        batch_size = predictions.shape[0]
        ndcg_sum = 0.0
        valid_users = 0
        
        for i in range(batch_size):
            pred = predictions[i]
            target = targets[i]
            
            # æ‰¾åˆ°æ­£æ ·æœ¬ä½ç½®
            positive_items = (target > 0).nonzero(as_tuple=False).squeeze(-1)
            if len(positive_items) == 0:
                continue
                
            # è·å–top-ké¢„æµ‹
            _, top_k_indices = torch.topk(pred, k=min(k, len(pred)))
            
            # è®¡ç®—DCG
            dcg = 0.0
            for j, item_idx in enumerate(top_k_indices):
                if item_idx in positive_items:
                    dcg += 1.0 / np.log2(j + 2)
            
            # è®¡ç®—IDCG
            num_relevant = min(len(positive_items), k)
            idcg = sum(1.0 / np.log2(j + 2) for j in range(num_relevant))
            
            # è®¡ç®—NDCG
            if idcg > 0:
                ndcg_sum += dcg / idcg
                valid_users += 1
        
        return ndcg_sum / valid_users if valid_users > 0 else 0.0
    
    def compute_hit_rate_at_k(self, predictions: torch.Tensor, targets: torch.Tensor, k: int = 10) -> float:
        """
        è®¡ç®—Hit Rate@K - åŸºäºUCRçš„è¯„ä¼°æ–¹æ³•
        Args:
            predictions: é¢„æµ‹åˆ†æ•° [batch_size, num_items]
            targets: çœŸå®æ ‡ç­¾ [batch_size, num_items]
            k: top-k
        Returns:
            Hit Rate@Kåˆ†æ•°
        """
        batch_size = predictions.shape[0]
        hit_sum = 0.0
        valid_users = 0
        
        for i in range(batch_size):
            pred = predictions[i]
            target = targets[i]
            
            # æ‰¾åˆ°æ­£æ ·æœ¬ä½ç½®
            positive_items = (target > 0).nonzero(as_tuple=False).squeeze(-1)
            if len(positive_items) == 0:
                continue
                
            # è·å–top-ké¢„æµ‹
            _, top_k_indices = torch.topk(pred, k=min(k, len(pred)))
            
            # æ£€æŸ¥æ˜¯å¦å‘½ä¸­
            hit = any(item_idx in positive_items for item_idx in top_k_indices)
            hit_sum += 1.0 if hit else 0.0
            valid_users += 1
        
        return hit_sum / valid_users if valid_users > 0 else 0.0
    
    def compute_counterfactual_fidelity(self, original_preds: torch.Tensor, 
                                      cf_preds: torch.Tensor, 
                                      explanations: List[List[int]], 
                                      k: int = 10) -> float:
        """
        è®¡ç®—åäº‹å®å¿ å®åº¦ - åŸºäºUCRçš„å¿ å®åº¦è¯„ä¼°
        Args:
            original_preds: åŸå§‹é¢„æµ‹ [batch_size, num_items]
            cf_preds: åäº‹å®é¢„æµ‹ [batch_size, num_items]
            explanations: è§£é‡Šåˆ—è¡¨ï¼Œæ¯ä¸ªç”¨æˆ·çš„é‡è¦ç‰©å“ä½ç½®
            k: top-k
        Returns:
            å¿ å®åº¦åˆ†æ•°
        """
        batch_size = original_preds.shape[0]
        fidelity_sum = 0.0
        valid_cases = 0
        
        for i in range(batch_size):
            if i >= len(explanations):
                continue
                
            orig_pred = original_preds[i]
            cf_pred = cf_preds[i]
            explanation = explanations[i]
            
            if len(explanation) == 0:
                continue
            
            # è·å–åŸå§‹top-k
            _, orig_top_k = torch.topk(orig_pred, k=min(k, len(orig_pred)))
            # è·å–åäº‹å®top-k
            _, cf_top_k = torch.topk(cf_pred, k=min(k, len(cf_pred)))
            
            # è®¡ç®—IoUï¼ˆäº¤å¹¶æ¯”ï¼‰
            orig_set = set(orig_top_k.cpu().numpy())
            cf_set = set(cf_top_k.cpu().numpy())
            
            intersection = len(orig_set.intersection(cf_set))
            union = len(orig_set.union(cf_set))
            
            if union > 0:
                iou = intersection / union
                # å¿ å®åº¦ = 1 - IoUï¼ˆå˜åŒ–è¶Šå¤§ï¼Œå¿ å®åº¦è¶Šé«˜ï¼‰
                fidelity = 1.0 - iou
                fidelity_sum += fidelity
                valid_cases += 1
        
        return fidelity_sum / valid_cases if valid_cases > 0 else 0.0
    
    def compute_explanation_sparsity(self, explanations: List[List[int]], 
                                   sequence_lengths: List[int]) -> float:
        """
        è®¡ç®—è§£é‡Šç¨€ç–æ€§
        Args:
            explanations: è§£é‡Šåˆ—è¡¨
            sequence_lengths: åºåˆ—é•¿åº¦åˆ—è¡¨
        Returns:
            ç¨€ç–æ€§åˆ†æ•°ï¼ˆè¶Šå°è¶Šç¨€ç–ï¼‰
        """
        if not explanations or not sequence_lengths:
            return 0.0
            
        sparsity_scores = []
        for explanation, seq_len in zip(explanations, sequence_lengths):
            if seq_len > 0:
                sparsity = len(explanation) / seq_len
                sparsity_scores.append(sparsity)
        
        return np.mean(sparsity_scores) if sparsity_scores else 0.0
    
    def compute_explanation_coverage(self, explanations: List[List[int]], 
                                   total_items: int) -> float:
        """
        è®¡ç®—è§£é‡Šè¦†ç›–ç‡
        Args:
            explanations: è§£é‡Šåˆ—è¡¨
            total_items: æ€»ç‰©å“æ•°
        Returns:
            è¦†ç›–ç‡åˆ†æ•°
        """
        if not explanations:
            return 0.0
            
        all_explained_items = set()
        for explanation in explanations:
            all_explained_items.update(explanation)
        
        coverage = len(all_explained_items) / total_items
        return coverage
    
    def compute_diversity_score(self, predictions: torch.Tensor, k: int = 10) -> float:
        """
        è®¡ç®—æ¨èå¤šæ ·æ€§åˆ†æ•°
        Args:
            predictions: é¢„æµ‹åˆ†æ•° [batch_size, num_items]
            k: top-k
        Returns:
            å¤šæ ·æ€§åˆ†æ•°
        """
        batch_size = predictions.shape[0]
        diversity_sum = 0.0
        
        for i in range(batch_size):
            pred = predictions[i]
            _, top_k_indices = torch.topk(pred, k=min(k, len(pred)))
            
            # è®¡ç®—top-kç‰©å“çš„æ–¹å·®ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
            top_k_scores = pred[top_k_indices]
            diversity = torch.std(top_k_scores).item()
            diversity_sum += diversity
        
        return diversity_sum / batch_size
    
    def update_metrics(self, original_preds: torch.Tensor, cf_preds: torch.Tensor,
                      targets: torch.Tensor, explanations: List[List[int]],
                      alignment_loss: float = 0.0, k: int = 10):
        """
        æ›´æ–°æ‰€æœ‰æŒ‡æ ‡
        Args:
            original_preds: åŸå§‹é¢„æµ‹
            cf_preds: åäº‹å®é¢„æµ‹
            targets: çœŸå®æ ‡ç­¾
            explanations: è§£é‡Šåˆ—è¡¨
            alignment_loss: å¯¹é½æŸå¤±
            k: top-k
        """
        # åŸºç¡€æ¨èæŒ‡æ ‡
        ndcg = self.compute_ndcg_at_k(original_preds, targets, k)
        hit_rate = self.compute_hit_rate_at_k(original_preds, targets, k)
        
        self.ndcg_scores.append(ndcg)
        self.hit_scores.append(hit_rate)
        
        # åäº‹å®æ¨èæŒ‡æ ‡
        cf_ndcg = self.compute_ndcg_at_k(cf_preds, targets, k)
        cf_hit_rate = self.compute_hit_rate_at_k(cf_preds, targets, k)
        
        self.cf_ndcg_scores.append(cf_ndcg)
        self.cf_hit_scores.append(cf_hit_rate)
        
        # æ”¹è¿›æŒ‡æ ‡
        improvement = cf_ndcg - ndcg
        self.improvement_scores.append(improvement)
        
        # å¿ å®åº¦æŒ‡æ ‡
        fidelity = self.compute_counterfactual_fidelity(original_preds, cf_preds, explanations, k)
        self.fidelity_scores.append(fidelity)
        
        # å¤šæ ·æ€§æŒ‡æ ‡
        diversity = self.compute_diversity_score(cf_preds, k)
        self.diversity_scores.append(diversity)
        
        # å¯¹é½æŸå¤±
        self.alignment_losses.append(alignment_loss)
        
        # è§£é‡Šè´¨é‡æŒ‡æ ‡
        if explanations:
            avg_length = np.mean([len(exp) for exp in explanations])
            self.explanation_lengths.append(avg_length)
    
    def get_summary_metrics(self) -> Dict[str, float]:
        """
        è·å–æ±‡æ€»æŒ‡æ ‡
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        summary = {}
        
        # åŸºç¡€æŒ‡æ ‡
        if self.ndcg_scores:
            summary['NDCG@10'] = np.mean(self.ndcg_scores)
            summary['Hit@10'] = np.mean(self.hit_scores)
        
        # åäº‹å®æŒ‡æ ‡
        if self.cf_ndcg_scores:
            summary['CF_NDCG@10'] = np.mean(self.cf_ndcg_scores)
            summary['CF_Hit@10'] = np.mean(self.cf_hit_scores)
        
        # æ”¹è¿›æŒ‡æ ‡
        if self.improvement_scores:
            summary['NDCG_Improvement'] = np.mean(self.improvement_scores)
            summary['Improvement_Std'] = np.std(self.improvement_scores)
        
        # å¿ å®åº¦å’Œå¤šæ ·æ€§
        if self.fidelity_scores:
            summary['Fidelity'] = np.mean(self.fidelity_scores)
        if self.diversity_scores:
            summary['Diversity'] = np.mean(self.diversity_scores)
        
        # å¯¹é½æŸå¤±
        if self.alignment_losses:
            summary['Alignment_Loss'] = np.mean(self.alignment_losses)
        
        # è§£é‡Šè´¨é‡
        if self.explanation_lengths:
            summary['Avg_Explanation_Length'] = np.mean(self.explanation_lengths)
        
        return summary
    
    def print_metrics_report(self):
        """æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š"""
        summary = self.get_summary_metrics()
        
        print("\n" + "="*60)
        print("CA-LLMRec åäº‹å®è¯„ä¼°æŠ¥å‘Š - Week 2")
        print("="*60)
        
        # åŸºç¡€æ¨èæ€§èƒ½
        print("\nğŸ“Š åŸºç¡€æ¨èæ€§èƒ½:")
        if 'NDCG@10' in summary:
            print(f"  NDCG@10: {summary['NDCG@10']:.4f}")
        if 'Hit@10' in summary:
            print(f"  Hit@10: {summary['Hit@10']:.4f}")
        
        # åäº‹å®æ¨èæ€§èƒ½
        print("\nğŸ”„ åäº‹å®æ¨èæ€§èƒ½:")
        if 'CF_NDCG@10' in summary:
            print(f"  CF_NDCG@10: {summary['CF_NDCG@10']:.4f}")
        if 'CF_Hit@10' in summary:
            print(f"  CF_Hit@10: {summary['CF_Hit@10']:.4f}")
        
        # æ”¹è¿›æ•ˆæœ
        print("\nğŸ“ˆ æ”¹è¿›æ•ˆæœ:")
        if 'NDCG_Improvement' in summary:
            print(f"  NDCGæ”¹è¿›: {summary['NDCG_Improvement']:.4f}")
            print(f"  æ”¹è¿›æ ‡å‡†å·®: {summary.get('Improvement_Std', 0):.4f}")
        
        # è§£é‡Šè´¨é‡
        print("\nğŸ” è§£é‡Šè´¨é‡:")
        if 'Fidelity' in summary:
            print(f"  å¿ å®åº¦: {summary['Fidelity']:.4f}")
        if 'Diversity' in summary:
            print(f"  å¤šæ ·æ€§: {summary['Diversity']:.4f}")
        if 'Avg_Explanation_Length' in summary:
            print(f"  å¹³å‡è§£é‡Šé•¿åº¦: {summary['Avg_Explanation_Length']:.2f}")
        
        # è®­ç»ƒè´¨é‡
        print("\nğŸ¯ è®­ç»ƒè´¨é‡:")
        if 'Alignment_Loss' in summary:
            print(f"  å¯¹é½æŸå¤±: {summary['Alignment_Loss']:.6f}")
        
        print("="*60)
    
    def save_metrics_to_file(self, filepath: str):
        """ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        summary = self.get_summary_metrics()
        
        # æ·»åŠ è¯¦ç»†æ•°æ®
        detailed_metrics = {
            'summary': summary,
            'detailed_scores': {
                'ndcg_scores': self.ndcg_scores,
                'hit_scores': self.hit_scores,
                'cf_ndcg_scores': self.cf_ndcg_scores,
                'cf_hit_scores': self.cf_hit_scores,
                'improvement_scores': self.improvement_scores,
                'fidelity_scores': self.fidelity_scores,
                'diversity_scores': self.diversity_scores,
                'alignment_losses': self.alignment_losses,
                'explanation_lengths': self.explanation_lengths
            }
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(detailed_metrics, f, indent=2, ensure_ascii=False)
        
        print(f"æŒ‡æ ‡å·²ä¿å­˜åˆ°: {filepath}")
    
    def plot_metrics_comparison(self, save_path: str = None):
        """ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾"""
        if not self.ndcg_scores or not self.cf_ndcg_scores:
            print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºç»˜å›¾")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # NDCGå¯¹æ¯”
        axes[0, 0].plot(self.ndcg_scores, label='Original NDCG', alpha=0.7)
        axes[0, 0].plot(self.cf_ndcg_scores, label='Counterfactual NDCG', alpha=0.7)
        axes[0, 0].set_title('NDCG@10 Comparison')
        axes[0, 0].set_xlabel('Batch')
        axes[0, 0].set_ylabel('NDCG@10')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ”¹è¿›åˆ†æ•°åˆ†å¸ƒ
        if self.improvement_scores:
            axes[0, 1].hist(self.improvement_scores, bins=20, alpha=0.7, color='green')
            axes[0, 1].set_title('NDCG Improvement Distribution')
            axes[0, 1].set_xlabel('NDCG Improvement')
            axes[0, 1].set_ylabel('Frequency')
            axes[0, 1].grid(True, alpha=0.3)
        
        # å¿ å®åº¦è¶‹åŠ¿
        if self.fidelity_scores:
            axes[1, 0].plot(self.fidelity_scores, color='orange', alpha=0.7)
            axes[1, 0].set_title('Counterfactual Fidelity Trend')
            axes[1, 0].set_xlabel('Batch')
            axes[1, 0].set_ylabel('Fidelity Score')
            axes[1, 0].grid(True, alpha=0.3)
        
        # å¯¹é½æŸå¤±è¶‹åŠ¿
        if self.alignment_losses:
            axes[1, 1].plot(self.alignment_losses, color='red', alpha=0.7)
            axes[1, 1].set_title('Alignment Loss Trend')
            axes[1, 1].set_xlabel('Batch')
            axes[1, 1].set_ylabel('Alignment Loss')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()


class CounterfactualEvaluator:
    """
    åäº‹å®è¯„ä¼°å™¨ - Week 2ä¸»è¦è¯„ä¼°æ¥å£
    """
    def __init__(self, model, device: str = 'cuda'):
        self.model = model
        self.device = device
        self.metrics = CounterfactualMetrics(device)
        
    def evaluate_model(self, dataloader, k: int = 10, 
                      max_batches: Optional[int] = None) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            k: top-k
            max_batches: æœ€å¤§è¯„ä¼°æ‰¹æ¬¡æ•°
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.model.eval()
        self.metrics.reset()
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if max_batches and batch_idx >= max_batches:
                    break
                
                # è·å–æ‰¹æ¬¡æ•°æ®
                user_ids = batch['user_ids']
                seqs = batch['seqs']
                pos_seqs = batch['pos_seqs']
                cf_seqs = batch['cf_seqs']
                
                # åŸå§‹é¢„æµ‹
                original_preds = self._get_predictions(user_ids, seqs, pos_seqs)
                
                # åäº‹å®é¢„æµ‹
                cf_preds = self._get_predictions(user_ids, cf_seqs, pos_seqs)
                
                # ç”Ÿæˆç›®æ ‡æ ‡ç­¾
                targets = self._generate_targets(pos_seqs)
                
                # è·å–è§£é‡Š
                explanations = self._get_explanations(seqs, batch.get('padding_ids', None))
                
                # è®¡ç®—å¯¹é½æŸå¤±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                alignment_loss = 0.0
                if hasattr(self.model, 'counterfactual_alignment_loss'):
                    # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å®ç°æ¥è°ƒæ•´
                    pass
                
                # æ›´æ–°æŒ‡æ ‡
                self.metrics.update_metrics(
                    original_preds, cf_preds, targets, 
                    explanations, alignment_loss, k
                )
        
        return self.metrics.get_summary_metrics()
    
    def _get_predictions(self, user_ids: torch.Tensor, seqs: torch.Tensor, 
                        pos_seqs: torch.Tensor) -> torch.Tensor:
        """è·å–æ¨¡å‹é¢„æµ‹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹å®ç°æ¥è°ƒæ•´
        # æš‚æ—¶è¿”å›éšæœºé¢„æµ‹ä½œä¸ºç¤ºä¾‹
        batch_size = seqs.shape[0]
        num_items = self.model.item_num if hasattr(self.model, 'item_num') else 1000
        return torch.randn(batch_size, num_items).to(self.device)
    
    def _generate_targets(self, pos_seqs: torch.Tensor) -> torch.Tensor:
        """ç”Ÿæˆç›®æ ‡æ ‡ç­¾"""
        batch_size = pos_seqs.shape[0]
        num_items = self.model.item_num if hasattr(self.model, 'item_num') else 1000
        targets = torch.zeros(batch_size, num_items).to(self.device)
        
        # å°†æ­£æ ·æœ¬ä½ç½®æ ‡è®°ä¸º1
        for i in range(batch_size):
            pos_items = pos_seqs[i][pos_seqs[i] > 0]
            if len(pos_items) > 0:
                targets[i, pos_items] = 1.0
        
        return targets
    
    def _get_explanations(self, seqs: torch.Tensor, 
                         padding_ids: Optional[torch.Tensor] = None) -> List[List[int]]:
        """è·å–è§£é‡Š"""
        explanations = []
        batch_size = seqs.shape[0]
        
        for i in range(batch_size):
            # è¿™é‡Œéœ€è¦æ ¹æ®åäº‹å®ç”Ÿæˆå™¨æ¥è·å–è§£é‡Š
            if hasattr(self.model, 'cf_generator'):
                important_positions, _ = self.model.cf_generator.get_explanation_weights(threshold=0.5)
                explanations.append(important_positions.cpu().numpy().tolist())
            else:
                explanations.append([])
        
        return explanations


def run_comprehensive_evaluation(model, dataloader, save_dir: str = './eval_results/'):
    """
    è¿è¡Œå…¨é¢çš„åäº‹å®è¯„ä¼°
    Args:
        model: å¾…è¯„ä¼°æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        save_dir: ç»“æœä¿å­˜ç›®å½•
    """
    import os
    os.makedirs(save_dir, exist_ok=True)
    
    print("ğŸš€ å¼€å§‹CA-LLMRecåäº‹å®è¯„ä¼°...")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CounterfactualEvaluator(model)
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate_model(dataloader, k=10, max_batches=100)
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.metrics.print_metrics_report()
    
    # ä¿å­˜ç»“æœ
    metrics_file = os.path.join(save_dir, 'counterfactual_metrics.json')
    evaluator.metrics.save_metrics_to_file(metrics_file)
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_file = os.path.join(save_dir, 'metrics_comparison.png')
    evaluator.metrics.plot_metrics_comparison(plot_file)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    
    return results 